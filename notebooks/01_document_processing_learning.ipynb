{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Document Processing for RAG System\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook explains the multilingual document processing pipeline - the first critical step in building a scalable multilingual RAG system:\n",
    "\n",
    "1. **Why language-aware document processing matters for RAG quality**\n",
    "2. **Text extraction from different file formats across languages**\n",
    "3. **Language-specific cleaning challenges (Croatian, English, extensible)**\n",
    "4. **Language-aware document chunking strategies and their impact**\n",
    "5. **Building a complete multilingual preprocessing pipeline**\n",
    "6. **Automatic language detection and folder organization**\n",
    "\n",
    "## 1. Why Multilingual Document Processing Matters\n",
    "\n",
    "### The Foundation of Cross-Language RAG Quality\n",
    "\n",
    "Document processing is like preparing ingredients for international cuisine - language-specific preparation ensures authentic results. In multilingual RAG systems:\n",
    "\n",
    "- **Language Preservation**: Maintain language-specific features (diacritics, scripts, etc.)\n",
    "- **Cross-Language Consistency**: Unified processing while respecting language differences\n",
    "- **Chunk Quality**: Language-aware splitting for better semantic coherence\n",
    "- **Metadata Enhancement**: Language tags and source information for better routing\n",
    "- **Encoding Robustness**: Handle various character encodings across languages\n",
    "\n",
    "### Language-Specific Processing Challenges\n",
    "\n",
    "#### üá≠üá∑ Croatian Language Challenges\n",
    "- **Diacritics**: ƒç, ƒá, ≈°, ≈æ, ƒë must be preserved correctly\n",
    "- **Encoding Issues**: Many documents use Windows-1250 or ISO-8859-2\n",
    "- **Morphological Complexity**: Rich inflectional system\n",
    "- **Regional Variations**: Different dialects and spellings\n",
    "\n",
    "#### üá¨üáß English Language Challenges  \n",
    "- **Business Documents**: Financial reports, legal contracts, technical specs\n",
    "- **Encoding Variations**: UTF-8, cp1252, iso-8859-1\n",
    "- **Technical Terminology**: Industry-specific vocabulary preservation\n",
    "- **Document Structure**: Complex layouts in business documents\n",
    "\n",
    "#### üåê Cross-Language Challenges\n",
    "- **Language Detection**: Automatic identification for proper routing\n",
    "- **Mixed Documents**: Handling multilingual content within single documents\n",
    "- **Unified Metadata**: Consistent tagging across language boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our multilingual document processing components\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from src.preprocessing.extractors import DocumentExtractor\n",
    "from src.preprocessing.cleaners import MultilingualTextCleaner\n",
    "from src.preprocessing.chunkers import DocumentChunker, chunk_document\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "print(\"‚úÖ Multilingual document processing components imported successfully!\")\n",
    "print(\"üåç Supporting Croatian, English, and extensible language framework\")\n",
    "print(\"üìÅ Language-aware processing with automatic detection capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilingual Document Organization\n",
    "\n",
    "### üìÅ Language-Based Folder Structure\n",
    "\n",
    "Our system uses a language-aware folder organization for scalable document management:\n",
    "\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hr/                    # Croatian documents\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NN-2025-115-1666.pdf\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ legal_document.docx\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ en/                    # English documents\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ financial_report.pdf\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ business_plan.docx\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ multilingual/          # Mixed-language documents\n",
    "‚îú‚îÄ‚îÄ processed/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hr/                    # Croatian processed chunks\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ en/                    # English processed chunks\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ shared/                # Cross-language resources\n",
    "‚îî‚îÄ‚îÄ test/\n",
    "    ‚îú‚îÄ‚îÄ hr/sample_croatian.txt\n",
    "    ‚îú‚îÄ‚îÄ en/sample_english.txt\n",
    "    ‚îî‚îÄ‚îÄ multilingual/mixed_content.txt\n",
    "```\n",
    "\n",
    "### üîÑ Language Detection and Routing\n",
    "\n",
    "1. **Automatic Detection**: Identify document language during ingestion\n",
    "2. **Folder Routing**: Move documents to appropriate language folders\n",
    "3. **Language-Specific Processing**: Apply language-aware cleaning and chunking\n",
    "4. **Unified Storage**: Store in multilingual vector database with language metadata\n",
    "\n",
    "## 3. Text Extraction from Different Formats\n",
    "\n",
    "Our system supports three main document types across all languages:\n",
    "\n",
    "### üìÑ Plain Text (.txt)\n",
    "- Simplest format but encoding can be tricky\n",
    "- **Croatian**: Often use Windows-1250 encoding, diacritics preservation\n",
    "- **English**: Usually UTF-8, but may encounter cp1252, iso-8859-1\n",
    "- **Detection**: UTF-8 detection and conversion is crucial\n",
    "\n",
    "### üìï PDF Documents (.pdf)\n",
    "- Complex format with fonts, images, layouts\n",
    "- May contain scanned text (OCR needed)\n",
    "- **Croatian**: Embedded font issues with diacritics\n",
    "- **English**: Business documents with complex layouts\n",
    "- **Multilingual**: Mixed-language content within single documents\n",
    "\n",
    "### üìò Word Documents (.docx)\n",
    "- Structured format with styles, headers\n",
    "- Generally good encoding support across languages\n",
    "- May contain tables, images, footnotes\n",
    "- **Language Detection**: Can identify primary language from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create sample multilingual documents for testing\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for our test documents\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=\"multilingual_docs_\"))\n",
    "print(f\"üìÅ Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Create language-specific subdirectories\n",
    "hr_dir = temp_dir / \"hr\"\n",
    "en_dir = temp_dir / \"en\"\n",
    "multilingual_dir = temp_dir / \"multilingual\"\n",
    "\n",
    "for lang_dir in [hr_dir, en_dir, multilingual_dir]:\n",
    "    lang_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created language directories: hr/, en/, multilingual/\")\n",
    "\n",
    "# Sample Croatian texts with various challenges\n",
    "croatian_texts = {\n",
    "    \"zagreb_info.txt\": \"\"\"\n",
    "Zagreb - Glavni Grad Hrvatske\n",
    "\n",
    "Zagreb je glavni i najveƒái grad Republike Hrvatske. Smje≈°ten je u sjeverozapadnom dijelu zemlje, na rijeci Savi.\n",
    "Grad ima bogatu povijest koja se≈æe u rimsko doba.\n",
    "\n",
    "TURISTIƒåKA MJESTA:\n",
    "‚Ä¢ Gornji grad - povijesni dio s crkvom sv. Marka\n",
    "‚Ä¢ Donji grad - trgovaƒçki i poslovni centar\n",
    "‚Ä¢ Maksimir - najveƒái park u Zagrebu\n",
    "\n",
    "Stanovni≈°tvo: ~800,000 stanovnika (2021.)\n",
    "Povr≈°ina: 641.4 km¬≤\n",
    "\n",
    "Zagreb je takoƒëer kulturno sredi≈°te Hrvatske s brojnim muzejima, kazali≈°tima i galerijama.\n",
    "\"\"\",\n",
    "\n",
    "    \"financije_hr.txt\": \"\"\"\n",
    "IZVJE≈†TAJ O FINANCIJSKIM REZULTATIMA\n",
    "\n",
    "Ukupni prihodi: 2.450.000,00 EUR\n",
    "Ukupni tro≈°kovi: 1.890.000,00 EUR\n",
    "DOBIT: 560.000,00 EUR\n",
    "\n",
    "Kljuƒçni pokazatelji:\n",
    "- Profitabilnost: 22.86%\n",
    "- ROI: 15.3%\n",
    "- Rast prihoda: +8.5% (godina)\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Sample English texts\n",
    "english_texts = {\n",
    "    \"business_report.txt\": \"\"\"\n",
    "QUARTERLY FINANCIAL REPORT - Q3 2025\n",
    "\n",
    "Executive Summary\n",
    "Our company achieved significant growth in Q3 2025, with total revenue reaching ‚Ç¨3,250,000.\n",
    "\n",
    "Key Performance Indicators:\n",
    "‚Ä¢ Revenue: ‚Ç¨3,250,000 (+12.5% YoY)\n",
    "‚Ä¢ Operating Profit: ‚Ç¨785,000 (+18.2% YoY)\n",
    "‚Ä¢ Net Margin: 24.15%\n",
    "‚Ä¢ Customer Acquisition: 1,847 new clients\n",
    "\n",
    "Market Analysis:\n",
    "The European market showed strong demand for our services, particularly in the technology and financial sectors.\n",
    "\n",
    "Outlook:\n",
    "We expect continued growth in Q4 2025, projecting revenues of ‚Ç¨3.8M.\n",
    "\"\"\",\n",
    "\n",
    "    \"technical_specs.txt\": \"\"\"\n",
    "SYSTEM SPECIFICATIONS\n",
    "\n",
    "Hardware Requirements:\n",
    "- CPU: Intel i7-12700K or AMD Ryzen 7 5800X\n",
    "- RAM: 32GB DDR4-3200\n",
    "- Storage: 1TB NVMe SSD\n",
    "- GPU: NVIDIA RTX 4070 (optional, for acceleration)\n",
    "\n",
    "Software Dependencies:\n",
    "- Python 3.11+\n",
    "- PyTorch 2.0+\n",
    "- Transformers 4.30+\n",
    "- ChromaDB 0.4+\n",
    "\n",
    "Performance Benchmarks:\n",
    "- Query Processing: <150ms\n",
    "- Document Ingestion: 2.5 docs/sec\n",
    "- Embedding Generation: 850 tokens/sec\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Mixed language document\n",
    "multilingual_texts = {\n",
    "    \"mixed_content.txt\": \"\"\"\n",
    "MULTILINGUAL DOCUMENT EXAMPLE\n",
    "\n",
    "English Section:\n",
    "This document demonstrates processing of mixed-language content.\n",
    "Total investment value: ‚Ç¨1,500,000\n",
    "\n",
    "Croatian Section:\n",
    "Ovaj dokument prikazuje obradu sadr≈æaja na vi≈°e jezika.\n",
    "Ukupna vrijednost investicije: 1.500.000,00 EUR\n",
    "\n",
    "Technical Details:\n",
    "- Processing language: Auto-detect\n",
    "- Encoding: UTF-8\n",
    "- Fallback: Language-specific processing\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write Croatian files\n",
    "for filename, content in croatian_texts.items():\n",
    "    file_path = hr_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"üá≠üá∑ Created Croatian file: {filename}\")\n",
    "\n",
    "# Write English files\n",
    "for filename, content in english_texts.items():\n",
    "    file_path = en_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"üá¨üáß Created English file: {filename}\")\n",
    "\n",
    "# Write multilingual files\n",
    "for filename, content in multilingual_texts.items():\n",
    "    file_path = multilingual_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"üåê Created multilingual file: {filename}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(croatian_texts)} Croatian, {len(english_texts)} English, and {len(multilingual_texts)} multilingual test documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test text extraction from our sample documents\n",
    "print(\"üìÇ DOCUMENT EXTRACTION TESTING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create document extractor\n",
    "extractor = DocumentExtractor()\n",
    "\n",
    "for filename, file_path in sample_files.items():\n",
    "    print(f\"\\nüìÑ Processing: {filename}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        # Extract text and metadata\n",
    "        result = extractor.extract_text(str(file_path))\n",
    "\n",
    "        print(f\"   ‚úÖ Extraction successful\")\n",
    "        print(f\"   üìä Text length: {len(result['content'])} characters\")\n",
    "        print(f\"   üè∑Ô∏è  File type: {result['metadata']['file_type']}\")\n",
    "        print(f\"   üìÖ File size: {result['metadata']['file_size']} bytes\")\n",
    "        print(f\"   üî§ Encoding: {result['metadata']['encoding']}\")\n",
    "\n",
    "        # Show first 150 characters\n",
    "        preview = result['content'][:150].replace('\\n', ' ')\n",
    "        print(f\"   üìñ Preview: {preview}...\")\n",
    "\n",
    "        # Check Croatian diacritics preservation\n",
    "        croatian_chars = ['ƒç', 'ƒá', '≈°', '≈æ', 'ƒë', 'ƒå', 'ƒÜ', '≈†', '≈Ω', 'ƒê']\n",
    "        found_diacritics = [char for char in croatian_chars if char in result['content']]\n",
    "        if found_diacritics:\n",
    "            print(f\"   ‚úì Croatian diacritics preserved: {', '.join(found_diacritics)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Extraction failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   ‚Ä¢ UTF-8 encoding preserves all Croatian diacritics\")\n",
    "print(\"   ‚Ä¢ Metadata extraction provides useful document information\")\n",
    "print(\"   ‚Ä¢ Text structure (headers, lists) is preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = DocumentChunker(config)\n",
    "\n",
    "try:\n",
    "    chunks = chunker.chunk_document(\n",
    "        text=test_document,\n",
    "        source_file=\"zagreb_analysis.txt\",\n",
    "        strategy=\"sliding_window\"\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Quality Analysis for {len(chunks)} chunks:\")\n",
    "    print()\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content = chunk.content\n",
    "        metadata = chunk.metadata\n",
    "\n",
    "        # Analyze chunk quality\n",
    "        sentences = content.count('. ') + content.count('!') + content.count('?')\n",
    "        words = len(content.split())\n",
    "        has_header = any(line.isupper() or line.startswith('#') for line in content.split('\\n'))\n",
    "\n",
    "        print(f\"\udcc4 Chunk {i+1} (ID: {metadata['chunk_id'][:8]}...):\")\n",
    "        print(f\"   üìè Length: {len(content)} chars, {words} words, ~{sentences} sentences\")\n",
    "        print(f\"   \udd22 Index: {metadata['chunk_index']}/{metadata['total_chunks']}\")\n",
    "        print(f\"   üè∑Ô∏è  Language: {metadata.get('language', 'auto-detected')}\")\n",
    "        print(f\"   üìÑ Source: {metadata['source_file']}\")\n",
    "        if has_header:\n",
    "            print(f\"   üìã Contains headers/structure\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in chunking: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Croatian Text Cleaning Challenges\n",
    "\n",
    "### Why Clean Text?\n",
    "\n",
    "Raw extracted text often contains:\n",
    "- **Formatting artifacts**: Extra spaces, line breaks\n",
    "- **Non-printable characters**: Control characters, BOM\n",
    "- **Inconsistent whitespace**: Tabs, multiple spaces\n",
    "- **OCR errors**: From scanned documents\n",
    "- **Mixed languages**: Headers/footers in other languages\n",
    "\n",
    "### Croatian-Specific Cleaning:\n",
    "\n",
    "1. **Diacritic Normalization**: Ensure consistent diacritic representation\n",
    "2. **Case Handling**: Proper Croatian title case rules\n",
    "3. **Punctuation**: Handle Croatian-specific quotation marks\n",
    "4. **Number Formats**: Croatian uses comma for decimals (123,45)\n",
    "5. **Date Formats**: DD.MM.YYYY. format common in Croatia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test Croatian text cleaning capabilities\n",
    "print(\"üßπ CROATIAN TEXT CLEANING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create text cleaner with Croatian-specific configuration\n",
    "cleaning_config = TextCleaningConfig(\n",
    "    normalize_whitespace=True,\n",
    "    remove_extra_newlines=True,\n",
    "    normalize_diacritics=False,  # Keep Croatian diacritics!\n",
    "    lowercase=False,  # Preserve proper names\n",
    "    remove_punctuation=False,\n",
    "    min_word_length=2,\n",
    "    language='hr'  # Croatian-specific rules\n",
    ")\n",
    "\n",
    "cleaner = CroatianTextCleaner(cleaning_config)\n",
    "\n",
    "print(f\"‚öôÔ∏è Text cleaner configured for Croatian:\")\n",
    "print(f\"   ‚Ä¢ Language: {cleaning_config.language}\")\n",
    "print(f\"   ‚Ä¢ Preserve diacritics: {not cleaning_config.normalize_diacritics}\")\n",
    "print(f\"   ‚Ä¢ Preserve case: {not cleaning_config.lowercase}\")\n",
    "print(f\"   ‚Ä¢ Min word length: {cleaning_config.min_word_length}\")\n",
    "\n",
    "# Test with messy Croatian text\n",
    "messy_text = \"\"\"\n",
    "\n",
    "   ZAGREB    -   GLAVNI GRAD\n",
    "\n",
    "\n",
    "Zagreb  je  glavni   grad  Republike   Hrvatske.\n",
    "    Nalazi    se   u   sjeverozapadnom    dijelu    zemlje.\n",
    "\n",
    "\n",
    "Stanovni≈°tvo:     ~800,000     stanovnika     (2021.)\n",
    "\n",
    "Povr≈°ina:   641.4    km¬≤\n",
    "\n",
    "\n",
    "VA≈ΩNE   ƒåINJENICE:\n",
    "‚Ä¢    Osnovan    je    u    11.    stoljeƒáu\n",
    "‚Ä¢  Glavni    grad   od    1991.  godine\n",
    "‚Ä¢    Sveuƒçili≈°te    osnovano    1669.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù Original messy text:\")\n",
    "print(f\"   Length: {len(messy_text)} characters\")\n",
    "print(f\"   Lines: {messy_text.count(chr(10))} newlines\")\n",
    "print(f\"   Preview: {repr(messy_text[:100])}...\")\n",
    "\n",
    "# Clean the text\n",
    "try:\n",
    "    cleaned_result = cleaner.clean_text(messy_text)\n",
    "    cleaned_text = cleaned_result['text']\n",
    "\n",
    "    print(\"\\n‚ú® Cleaned text:\")\n",
    "    print(f\"   Length: {len(cleaned_text)} characters (reduced by {len(messy_text) - len(cleaned_text)})\")\n",
    "    print(f\"   Lines: {cleaned_text.count(chr(10))} newlines\")\n",
    "    print(f\"   Preview: {cleaned_text[:200]}...\")\n",
    "\n",
    "    # Show cleaning statistics\n",
    "    stats = cleaned_result['metadata']['cleaning_stats']\n",
    "    print(\"\\nüìä Cleaning Statistics:\")\n",
    "    for operation, count in stats.items():\n",
    "        if count > 0:\n",
    "            print(f\"   ‚Ä¢ {operation}: {count}\")\n",
    "\n",
    "    # Verify Croatian diacritics are preserved\n",
    "    croatian_chars = ['ƒç', 'ƒá', '≈°', '≈æ', 'ƒë']\n",
    "    found_chars = [char for char in croatian_chars if char in cleaned_text]\n",
    "    if found_chars:\n",
    "        print(f\"\\n‚úì Croatian diacritics preserved: {', '.join(found_chars)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cleaning failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = \"paragraph\"\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = \"sentence\"\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = \"sliding_window\"  # Preserve structure\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = \"sentence\"\n",
    "        max_size = 200\n",
    "\n",
    "    chunker = DocumentChunker(language='hr')\n",
    "\n",
    "    try:\n",
    "        chunks = chunker.chunk_document(\n",
    "            text=content.strip(),\n",
    "            source_file=f\"{content_name.lower()}_sample.txt\",\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        print(f\"   üìä Strategy: {strategy}, Max size: {max_size}\")\n",
    "        print(f\"   üìë Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk.content[:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk.content)} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Chunking Strategies\n",
    "\n",
    "### Why Chunk Documents?\n",
    "\n",
    "Large documents must be split into smaller pieces because:\n",
    "- **Embedding limits**: Models have max token limits (512-1024 tokens)\n",
    "- **Search precision**: Smaller chunks = more focused results\n",
    "- **Context relevance**: Large chunks may contain irrelevant information\n",
    "- **Processing efficiency**: Smaller pieces are faster to process\n",
    "\n",
    "### Chunking Strategies:\n",
    "\n",
    "1. **Sentence-based**: Split on sentence boundaries (good for Croatian)\n",
    "2. **Fixed-size**: Split by character/token count\n",
    "3. **Paragraph-based**: Split on paragraph breaks\n",
    "4. **Semantic**: Split based on topic/meaning\n",
    "5. **Hybrid**: Combine multiple approaches\n",
    "\n",
    "### Croatian Considerations:\n",
    "\n",
    "- **Sentence detection**: Croatian punctuation patterns\n",
    "- **Long sentences**: Croatian can have very long complex sentences\n",
    "- **Paragraph structure**: Formal vs informal text differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        # Use chunk_document with source_file parameter\n",
    "        chunks = chunker.chunk_document(\n",
    "            content=content.strip(),\n",
    "            source_file=f\"sample_{content_name.lower()}.txt\",\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   üìä Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   üìë Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        # Use chunk_document with source_file parameter\n",
    "        chunks = chunker.chunk_document(\n",
    "            content=content.strip(),\n",
    "            source_file=f\"sample_{content_name.lower()}.txt\",\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   \udcca Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   üìë Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking with different types of Croatian content\n",
    "print(\"üìö CHUNKING DIFFERENT CONTENT TYPES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Different Croatian content types\n",
    "content_types = {\n",
    "    \"News Article\": \"\"\"\n",
    "ZAGREB, 15. srpnja 2023. - Hrvatski premijer najavio je nova ulaganja u infrastrukturu.\n",
    "\n",
    "Prema rijeƒçima premijera, projekti ƒáe ukljuƒçivati modernizaciju cesta, ≈æeljeznica i digitalne infrastrukture. \"Ovo su strate≈°ka ulaganja za buduƒánost Hrvatske\", rekao je premijer na tiskovnoj konferenciji.\n",
    "\n",
    "Ukupna vrijednost projekata procjenjuje se na 2,5 milijardi eura. Financiranje ƒáe biti osigurano iz EU fondova i dr≈æavnog proraƒçuna.\n",
    "\"\"\",\n",
    "\n",
    "    \"Academic Text\": \"\"\"\n",
    "Sintaksa hrvatskog jezika odlikuje se slo≈æeno≈°ƒáu koja proizlazi iz bogate morfologije. Fleksijska priroda jezika omoguƒáuje relativno slobodan red rijeƒçi, ≈°to se posebno oƒçituje u poetskom diskursu.\n",
    "\n",
    "Slavenski jezici, ukljuƒçujuƒái hrvatski, karakterizira razvijen aspektualnost glagolski sistem. Perfektivnost i imperfektivnost glagola fundamentalne su kategorije koje utjeƒçu na temporalnu strukturu iskaza.\n",
    "\"\"\",\n",
    "\n",
    "    \"Recipe\": \"\"\"\n",
    "SARMA - tradicionalno zimsko jelo\n",
    "\n",
    "Potrebno:\n",
    "‚Ä¢ 1 kg mije≈°anog mesa\n",
    "‚Ä¢ 1 glavica kiselog kupusa\n",
    "‚Ä¢ 200g ri≈æe\n",
    "‚Ä¢ 2 luka\n",
    "‚Ä¢ Sol, papar, vegeta\n",
    "\n",
    "Priprema:\n",
    "1. Prokuhajte ri≈æu na pola\n",
    "2. Pomije≈°ajte meso s ri≈æom i zaƒçinima\n",
    "3. Zamotajte u kupusove listove\n",
    "4. Kuhajte 2 sata na laganoj vatri\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for content_name, content in content_types.items():\n",
    "    print(f\"üìñ {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        chunks = chunker.chunk_text(\n",
    "            text=content.strip(),\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   üìä Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   üìë Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"üéØ Content-specific chunking improves RAG quality:\")\n",
    "print(\"   ‚Ä¢ News: Paragraph-based preserves story flow\")\n",
    "print(\"   ‚Ä¢ Academic: Sentence-based handles complexity\")\n",
    "print(\"   ‚Ä¢ Recipes: Hybrid preserves structured lists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Document Processing Pipeline\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "A complete document processing pipeline combines:\n",
    "1. **Document detection and loading**\n",
    "2. **Format-specific text extraction**\n",
    "3. **Croatian-aware text cleaning**\n",
    "4. **Intelligent document chunking**\n",
    "5. **Metadata preservation and enrichment**\n",
    "\n",
    "### Pipeline Benefits:\n",
    "- **Consistency**: Same processing for all documents\n",
    "- **Quality**: Each step improves text quality\n",
    "- **Scalability**: Can process hundreds of documents\n",
    "- **Traceability**: Track processing steps and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_complete(file_path, chunk_strategy=\"sentence\"):\n",
    "    \"\"\"Complete document processing pipeline.\"\"\"\n",
    "    print(f\"   \udcc4 Processing: {Path(file_path).name}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        print(\"   1Ô∏è‚É£ Text extraction...\")\n",
    "        extractor = DocumentExtractor()\n",
    "        raw_text, file_metadata = extractor.extract_text(file_path)\n",
    "        print(f\"      ‚úÖ Extracted {len(raw_text)} characters\")\n",
    "\n",
    "        # Step 2: Clean text\n",
    "        print(\"   2Ô∏è‚É£ Text cleaning...\")\n",
    "        cleaner = MultilingualTextCleaner(language='hr')\n",
    "        cleaned_text, cleaning_stats = cleaner.clean_text(raw_text)\n",
    "        print(f\"      ‚úÖ Cleaned: {cleaning_stats['characters_removed']} chars removed\")\n",
    "\n",
    "        # Step 3: Chunk text\n",
    "        print(\"   3Ô∏è‚É£ Text chunking...\")\n",
    "\n",
    "        # Map strategy names to valid options\n",
    "        strategy_mapping = {\n",
    "            \"sentence\": \"sentence\",\n",
    "            \"paragraph\": \"paragraph\",\n",
    "            \"sliding_window\": \"sliding_window\"\n",
    "        }\n",
    "\n",
    "        strategy = strategy_mapping.get(chunk_strategy, \"sentence\")\n",
    "        chunker = DocumentChunker(language='hr')\n",
    "\n",
    "        chunks = chunker.chunk_document(\n",
    "            text=cleaned_text,\n",
    "            source_file=str(file_path),\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        print(f\"      ‚úÖ Created {len(chunks)} chunks\")\n",
    "\n",
    "        # Step 4: Quality check\n",
    "        print(\"   4Ô∏è‚É£ Quality check...\")\n",
    "        chunk_sizes = [len(chunk.content) for chunk in chunks]\n",
    "        avg_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0\n",
    "\n",
    "        # Check for quality issues\n",
    "        quality_issues = []\n",
    "        if avg_size < 50:\n",
    "            quality_issues.append(\"chunks too small\")\n",
    "        if max(chunk_sizes) if chunk_sizes else 0 > 500:\n",
    "            quality_issues.append(\"some chunks too large\")\n",
    "        if not any('ƒç' in chunk.content or 'ƒá' in chunk.content or\n",
    "                  '≈°' in chunk.content or '≈æ' in chunk.content or\n",
    "                  'ƒë' in chunk.content for chunk in chunks):\n",
    "            if any(c in raw_text for c in 'ƒçƒá≈°≈æƒë'):\n",
    "                quality_issues.append(\"Croatian diacritics lost\")\n",
    "\n",
    "        if quality_issues:\n",
    "            print(f\"      ‚ö†Ô∏è Quality issues: {', '.join(quality_issues)}\")\n",
    "        else:\n",
    "            print(f\"      ‚úÖ Quality check passed\")\n",
    "\n",
    "        print(f\"      üìä Avg chunk size: {avg_size:.1f} chars\")\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Pipeline failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Multilingual Document Examples\n",
    "\n",
    "### üåç Real-World Document Processing Scenarios\n",
    "\n",
    "Here we demonstrate processing diverse multilingual documents that reflect actual use cases in Croatian business and government environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced multilingual document examples for processing\n",
    "multilingual_documents = {\n",
    "    \"üèõÔ∏è Croatian Legal Document\": {\n",
    "        \"content\": \"\"\"ODLUKA VLADE REPUBLIKE HRVATSKE\n",
    "O IZMJENI ZAKONA O RADU\n",
    "\n",
    "Klasa: 022-03/25-01/89\n",
    "Urbroj: 50301-01-25-4\n",
    "Zagreb, 15. lipnja 2025.\n",
    "\n",
    "Na temelju ƒçlanka 89. Ustava Republike Hrvatske, Vlada Republike Hrvatske na sjednici odr≈æanoj 15. lipnja 2025. godine donosi\n",
    "\n",
    "ODLUKU\n",
    "O IZMJENI ZAKONA O RADU\n",
    "\n",
    "ƒålanak 1.\n",
    "U Zakonu o radu (¬ªNarodne novine¬´, broj 93/14, 127/17, 98/19 i 151/22) mijenja se ƒçlanak 142. koji glasi:\n",
    "\n",
    "¬ªMinimalna mjeseƒçna plaƒáa za puno radno vrijeme iznosi 721,79 EUR.\n",
    "Minimalna mjeseƒçna plaƒáa uvjetovana je indeksom potro≈°aƒçkih cijena.\n",
    "Vlada Republike Hrvatske donosi uredbu o visini minimalne plaƒáe.¬´\n",
    "\n",
    "ƒålanak 2.\n",
    "Ova Odluka stupa na snagu 1. srpnja 2025. godine.\n",
    "\n",
    "Predsjednik Vlade Republike Hrvatske\n",
    "Andrej Plenkoviƒá\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"legal\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"formal_language\", \"legal_terminology\", \"official_formatting\", \"currency_amounts\", \"dates\"]\n",
    "    },\n",
    "\n",
    "    \"üíº Croatian Business Report\": {\n",
    "        \"content\": \"\"\"IZVJE≈†ƒÜE O POSLOVANJU ZA Q3 2025\n",
    "TECH SOLUTIONS d.o.o.\n",
    "\n",
    "SA≈ΩETAK FINANCIJSKIH REZULTATA\n",
    "\n",
    "Ukupni prihodi: 2.450.000,00 EUR (+15,3% u odnosu na Q3 2024)\n",
    "Neto dobit: 367.500,00 EUR (+22,7% YoY)\n",
    "EBITDA: 490.000,00 EUR (20,0% mar≈æa)\n",
    "\n",
    "KLJUƒåNI INDIKATORI PERFORMANSI (KPI)\n",
    "‚Ä¢ Broj zaposlenih: 47 (+6 nova zaposlenja)\n",
    "‚Ä¢ Customer Satisfaction Score: 4.7/5.0\n",
    "‚Ä¢ Net Promoter Score (NPS): +65\n",
    "‚Ä¢ Retention rate: 94,2%\n",
    "\n",
    "PROJEKTI I INOVACIJE\n",
    "Tijekom Q3 implementirali smo RAG (Retrieval-Augmented Generation) sustav za automatizaciju customer support-a. Sustav koristi BGE-M3 embeddings i Ollama lokalnu infrastrukturu.\n",
    "\n",
    "Rezultati:\n",
    "- 40% smanjenje response time-a\n",
    "- 15% poveƒáanje customer satisfaction\n",
    "- U≈°teda od 25.000 EUR mjeseƒçno na operativnim tro≈°kovima\n",
    "\n",
    "OUTLOOK ZA Q4 2025\n",
    "Oƒçekujemo daljnji rast prihoda od 12-18% te lansiranje novog AI-powered produkta.\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"business\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"features\": [\"mixed_terminology\", \"financial_data\", \"percentages\", \"technical_terms\", \"english_acronyms\"]\n",
    "    },\n",
    "\n",
    "    \"üî¨ English Technical Documentation\": {\n",
    "        \"content\": \"\"\"RAG SYSTEM ARCHITECTURE SPECIFICATION\n",
    "Version 2.1.3 | Build Date: September 5, 2025\n",
    "\n",
    "OVERVIEW\n",
    "This document outlines the technical architecture for a multilingual Retrieval-Augmented Generation (RAG) system supporting Croatian and English languages.\n",
    "\n",
    "SYSTEM COMPONENTS\n",
    "\n",
    "1. DOCUMENT PROCESSING PIPELINE\n",
    "   - Input formats: PDF, DOCX, TXT\n",
    "   - Encoding detection: UTF-8, Windows-1250, ISO-8859-2\n",
    "   - Language detection: Pattern-based Croatian/English classification\n",
    "   - Text extraction: pypdf, python-docx libraries\n",
    "   - Cleaning: Unicode normalization, whitespace handling\n",
    "\n",
    "2. CHUNKING STRATEGY\n",
    "   - Sentence-based: 100-400 characters with 20-50 overlap\n",
    "   - Paragraph-based: Semantic boundary preservation\n",
    "   - Hybrid: Content-aware splitting for complex documents\n",
    "   - Language-aware: Croatian morphology considerations\n",
    "\n",
    "3. VECTOR DATABASE\n",
    "   - Embeddings: BGE-M3 (1024 dimensions)\n",
    "   - Storage: ChromaDB with persistence\n",
    "   - Indexing: HNSW algorithm for similarity search\n",
    "   - Metadata: Language tags, source references, timestamps\n",
    "\n",
    "4. RETRIEVAL SYSTEM\n",
    "   - Dense retrieval: Semantic similarity via embeddings\n",
    "   - Sparse retrieval: BM25 with multilingual tokenization\n",
    "   - Hybrid fusion: Weighted combination (0.7 dense + 0.3 sparse)\n",
    "   - Reranking: Cross-encoder for result refinement\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "- Query latency: <200ms (p95)\n",
    "- Retrieval accuracy: >85% (HR), >92% (EN)\n",
    "- Storage efficiency: 4.2MB per 1000 documents\n",
    "- Throughput: 15 queries/second sustained\"\"\",\n",
    "        \"language\": \"en\",\n",
    "        \"domain\": \"technical\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"technical_specifications\", \"performance_metrics\", \"system_architecture\", \"acronyms\", \"version_numbers\"]\n",
    "    },\n",
    "\n",
    "    \"üåç Mixed Language Business Communication\": {\n",
    "        \"content\": \"\"\"MEETINGR ZAPISNIK / MEETING MINUTES\n",
    "Tech Solutions d.o.o. - Q3 Strategy Review\n",
    "Datum/Date: 15. rujan 2025. / September 15, 2025\n",
    "\n",
    "SUDIONICI/ATTENDEES:\n",
    "- Marko Horvat (CEO)\n",
    "- Sarah Johnson (CTO)\n",
    "- Ana Novak (Head of Operations)\n",
    "- James Wilson (Product Manager)\n",
    "\n",
    "AGENDA ITEMS:\n",
    "\n",
    "1. Q3 PERFORMANCE REVIEW\n",
    "Marko: \"Rezultati za Q3 su odliƒçni - 15.3% rast u odnosu na pro≈°lu godinu.\"\n",
    "Sarah: \"The RAG implementation exceeded our expectations. Response time improved by 40%.\"\n",
    "\n",
    "2. TECHNICAL ROADMAP\n",
    "Sarah: \"We need to expand our LLM capabilities. Predla≈æem da implementiramo nova multilingual models.\"\n",
    "James: \"I agree. Our customer base is 60% Croatian, 35% English, 5% other languages.\"\n",
    "\n",
    "3. RESOURCE ALLOCATION\n",
    "Ana: \"Za Q4 trebamo zaposliti jo≈° 3 developera i 2 data scientista.\"\n",
    "Sarah: \"Budget approval needed for additional GPU infrastructure - approximately ‚Ç¨45,000.\"\n",
    "\n",
    "4. NEXT STEPS\n",
    "- Implement BGE-M3 embeddings for better Croatian support\n",
    "- Hire additional technical staff (deadline: 30. listopad 2025.)\n",
    "- Present findings to board (meeting scheduled November 12th, 2025)\n",
    "\n",
    "CLOSING REMARKS:\n",
    "Marko: \"Excellent progress team. Let's maintain this momentum kroz Q4.\"\n",
    "\n",
    "SLJEDEƒÜI SASTANAK/NEXT MEETING: 30. rujan 2025. / September 30, 2025\"\"\",\n",
    "        \"language\": \"mixed\",\n",
    "        \"domain\": \"business_communication\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"features\": [\"code_switching\", \"bilingual_formatting\", \"meeting_structure\", \"mixed_dates\", \"currency_amounts\"]\n",
    "    },\n",
    "\n",
    "    \"üìä Croatian Government Statistics\": {\n",
    "        \"content\": \"\"\"DR≈ΩAVNI ZAVOD ZA STATISTIKU REPUBLIKE HRVATSKE\n",
    "STATISTIƒåKO IZVJE≈†ƒÜE BR. 1.2.3/2025\n",
    "\n",
    "DEMOGRAFSKI TRENDOVI U HRVATSKOJ - RUJAN 2025\n",
    "\n",
    "STANOVNI≈†TVO\n",
    "Ukupno stanovni≈°tvo: 3.871.833 (-0,8% u odnosu na 2024.)\n",
    "Prirodni prirast: -1,2 ‚Ä∞\n",
    "Migracijski saldo: +0,4 ‚Ä∞\n",
    "\n",
    "REGIONALNA DISTRIBUCIJA\n",
    "‚Ä¢ Zagreb (grad): 769.944 stanovnika (+0,2%)\n",
    "‚Ä¢ Split-Dalmatinska ≈æupanija: 454.798 (-1,1%)\n",
    "‚Ä¢ Primorsko-goranska ≈æupanija: 296.195 (-0,9%)\n",
    "‚Ä¢ Osjeƒçko-baranjska ≈æupanija: 305.032 (-1,4%)\n",
    "\n",
    "EKONOMSKI INDIKATORI\n",
    "Prosjeƒçna neto plaƒáa: 1.235,67 EUR (+7,2% YoY)\n",
    "Stopa nezaposlenosti: 6,1% (-0,8 postotnih bodova)\n",
    "Inflacija (mjeseƒçno): 2,4%\n",
    "BDP per capita: 16.247 EUR\n",
    "\n",
    "OBRAZOVANJE\n",
    "Stopa pismenosti: 99,2%\n",
    "Visoko≈°kolska naobrazba: 28,7% (+1,2 p.p.)\n",
    "STEM diplomanti: 23,4% ukupnih diplomanata\n",
    "\n",
    "NAPOMENE METODOLOGIJE\n",
    "Podaci se temelje na registru stanovni≈°tva i anketi radne snage.\n",
    "Confidence interval: 95%\n",
    "Margin of error: ¬±0,3%\n",
    "\n",
    "Objavljeno: 5. rujan 2025.\n",
    "Sljedeƒáe izvje≈°ƒáe: 5. prosinac 2025.\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"statistics\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"statistical_data\", \"percentages\", \"regional_data\", \"economic_indicators\", \"methodological_notes\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üåç Advanced Multilingual Document Examples Prepared:\")\n",
    "print(\"=\" * 65)\n",
    "for doc_name, doc_data in multilingual_documents.items():\n",
    "    print(f\"{doc_name}\")\n",
    "    print(f\"   Language: {doc_data['language']} | Domain: {doc_data['domain']}\")\n",
    "    print(f\"   Complexity: {doc_data['complexity']} | Length: {len(doc_data['content'])} chars\")\n",
    "    print(f\"   Features: {', '.join(doc_data['features'])}\")\n",
    "    print(f\"   Preview: {doc_data['content'][:80].replace(chr(10), ' ')}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive multilingual document processing demonstration\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def detect_document_language(text):\n",
    "    \"\"\"Simple language detection for demonstration.\"\"\"\n",
    "    croatian_indicators = len(re.findall(r'[ƒçƒá≈æ≈°ƒë]', text.lower()))\n",
    "    croatian_words = len(re.findall(r'\\b(je|su|za|na|u|od|do|se|i|a|koji|koja|koje|gdje|kada|kako|≈°to)\\b', text.lower()))\n",
    "    english_indicators = len(re.findall(r'\\b(the|and|or|of|in|to|for|with|on|at|by|from)\\b', text.lower()))\n",
    "\n",
    "    total_words = len(text.split())\n",
    "    if total_words == 0:\n",
    "        return \"unknown\"\n",
    "\n",
    "    croatian_score = (croatian_indicators * 3 + croatian_words) / total_words\n",
    "    english_score = english_indicators / total_words\n",
    "\n",
    "    if croatian_score > 0.1 and english_score > 0.05:\n",
    "        return \"mixed\"\n",
    "    elif croatian_score > english_score:\n",
    "        return \"hr\"\n",
    "    elif english_score > 0.05:\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def extract_document_features(text, expected_features):\n",
    "    \"\"\"Extract and validate document features.\"\"\"\n",
    "    found_features = []\n",
    "\n",
    "    # Check for each expected feature\n",
    "    if \"formal_language\" in expected_features:\n",
    "        formal_patterns = r'\\b(Vlada|Republika|Odluka|ƒçlanak|temelju|Ustava)\\b'\n",
    "        if re.search(formal_patterns, text, re.IGNORECASE):\n",
    "            found_features.append(\"formal_language\")\n",
    "\n",
    "    if \"financial_data\" in expected_features:\n",
    "        financial_patterns = r'\\d+[.,]\\d+\\s*(EUR|‚Ç¨|\\%)'\n",
    "        if re.search(financial_patterns, text):\n",
    "            found_features.append(\"financial_data\")\n",
    "\n",
    "    if \"technical_terms\" in expected_features:\n",
    "        tech_patterns = r'\\b(RAG|API|BGE-M3|embeddings|vector|database|algorithm|pipeline)\\b'\n",
    "        if re.search(tech_patterns, text, re.IGNORECASE):\n",
    "            found_features.append(\"technical_terms\")\n",
    "\n",
    "    if \"dates\" in expected_features:\n",
    "        date_patterns = r'\\d{1,2}\\.\\s*(sijeƒçnja|veljaƒçe|o≈æujka|travnja|svibnja|lipnja|srpnja|kolovoza|rujna|listopada|studenoga|prosinca|\\d+)\\s*\\d{4}|\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s*\\d{4}'\n",
    "        if re.search(date_patterns, text):\n",
    "            found_features.append(\"dates\")\n",
    "\n",
    "    if \"mixed_terminology\" in expected_features:\n",
    "        mixed_patterns = r'\\b(customer|support|response|time|satisfaction|score)\\b.*\\b(sustav|implementacija|rezultati|tro≈°kovi)\\b|\\b(sustav|implementacija|rezultati|tro≈°kovi)\\b.*\\b(customer|support|response|time|satisfaction|score)\\b'\n",
    "        if re.search(mixed_patterns, text, re.IGNORECASE | re.DOTALL):\n",
    "            found_features.append(\"mixed_terminology\")\n",
    "\n",
    "    if \"code_switching\" in expected_features:\n",
    "        # Look for language switches within sentences\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            croatian_words = len(re.findall(r'\\b(je|su|za|na|u|od|trebamo|mo≈æemo|godina)\\b', sentence.lower()))\n",
    "            english_words = len(re.findall(r'\\b(the|and|we|need|can|year|meeting|next)\\b', sentence.lower()))\n",
    "            if croatian_words > 0 and english_words > 0:\n",
    "                found_features.append(\"code_switching\")\n",
    "                break\n",
    "\n",
    "    return found_features\n",
    "\n",
    "# Process all multilingual documents\n",
    "print(\"üìä Comprehensive Multilingual Document Processing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "processing_results = []\n",
    "\n",
    "for doc_name, doc_data in multilingual_documents.items():\n",
    "    print(f\"\\nüîç Processing: {doc_name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    content = doc_data['content']\n",
    "    expected_lang = doc_data['language']\n",
    "    expected_features = doc_data['features']\n",
    "\n",
    "    # Language detection\n",
    "    detected_lang = detect_document_language(content)\n",
    "    lang_match = detected_lang == expected_lang\n",
    "\n",
    "    # Feature extraction\n",
    "    found_features = extract_document_features(content, expected_features)\n",
    "    feature_accuracy = len(found_features) / len(expected_features) if expected_features else 0\n",
    "\n",
    "    # Document statistics\n",
    "    word_count = len(content.split())\n",
    "    char_count = len(content)\n",
    "    sentence_count = len([s for s in content.split('.') if s.strip()])\n",
    "\n",
    "    # Chunking simulation (sentence-based)\n",
    "    sentences = [s.strip() + '.' for s in content.split('.') if s.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk + sentence) <= 300:  # Target chunk size\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'name': doc_name,\n",
    "        'expected_lang': expected_lang,\n",
    "        'detected_lang': detected_lang,\n",
    "        'lang_match': lang_match,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'chunk_count': len(chunks),\n",
    "        'expected_features': expected_features,\n",
    "        'found_features': found_features,\n",
    "        'feature_accuracy': feature_accuracy,\n",
    "        'domain': doc_data['domain'],\n",
    "        'complexity': doc_data['complexity']\n",
    "    }\n",
    "    processing_results.append(result)\n",
    "\n",
    "    # Display results\n",
    "    status = \"‚úÖ\" if lang_match else \"‚ùå\"\n",
    "    print(f\"{status} Language Detection: {detected_lang} (expected: {expected_lang})\")\n",
    "    print(f\"üìè Document Stats: {word_count} words, {sentence_count} sentences, {char_count} chars\")\n",
    "    print(f\"üî™ Chunking Result: {len(chunks)} chunks created\")\n",
    "    print(f\"üè∑Ô∏è  Feature Accuracy: {feature_accuracy:.1%} ({len(found_features)}/{len(expected_features)})\")\n",
    "    print(f\"   Expected: {', '.join(expected_features[:3])}{'...' if len(expected_features) > 3 else ''}\")\n",
    "    print(f\"   Found: {', '.join(found_features[:3])}{'...' if len(found_features) > 3 else ''}\")\n",
    "\n",
    "    # Show sample chunk\n",
    "    if chunks:\n",
    "        sample_chunk = chunks[0][:100] + \"...\" if len(chunks[0]) > 100 else chunks[0]\n",
    "        print(f\"üìÑ Sample Chunk: {sample_chunk}\")\n",
    "\n",
    "# Overall processing statistics\n",
    "print(f\"\\nüéØ Overall Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_docs = len(processing_results)\n",
    "correct_lang_detection = sum(1 for r in processing_results if r['lang_match'])\n",
    "avg_feature_accuracy = sum(r['feature_accuracy'] for r in processing_results) / total_docs\n",
    "total_chunks = sum(r['chunk_count'] for r in processing_results)\n",
    "avg_chunk_per_doc = total_chunks / total_docs\n",
    "\n",
    "print(f\"üìä Language Detection Accuracy: {correct_lang_detection}/{total_docs} ({correct_lang_detection/total_docs:.1%})\")\n",
    "print(f\"üéØ Average Feature Detection: {avg_feature_accuracy:.1%}\")\n",
    "print(f\"üî™ Total Chunks Created: {total_chunks}\")\n",
    "print(f\"üìà Average Chunks per Document: {avg_chunk_per_doc:.1f}\")\n",
    "\n",
    "# Domain and complexity analysis\n",
    "domain_stats = Counter(r['domain'] for r in processing_results)\n",
    "complexity_stats = Counter(r['complexity'] for r in processing_results)\n",
    "\n",
    "print(f\"\\nüìÇ Domain Distribution: {dict(domain_stats)}\")\n",
    "print(f\"‚öñÔ∏è  Complexity Distribution: {dict(complexity_stats)}\")\n",
    "\n",
    "# Language-specific performance\n",
    "lang_performance = {}\n",
    "for result in processing_results:\n",
    "    lang = result['expected_lang']\n",
    "    if lang not in lang_performance:\n",
    "        lang_performance[lang] = {'correct': 0, 'total': 0, 'chunks': 0}\n",
    "    lang_performance[lang]['total'] += 1\n",
    "    lang_performance[lang]['chunks'] += result['chunk_count']\n",
    "    if result['lang_match']:\n",
    "        lang_performance[lang]['correct'] += 1\n",
    "\n",
    "print(f\"\\nüåç Language-Specific Performance:\")\n",
    "for lang, stats in lang_performance.items():\n",
    "    accuracy = stats['correct'] / stats['total']\n",
    "    avg_chunks = stats['chunks'] / stats['total']\n",
    "    print(f\"   {lang}: {accuracy:.1%} accuracy, {avg_chunks:.1f} avg chunks/doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "print(\"\ude80 Testing complete pipeline on our sample documents:\")\n",
    "print()\n",
    "\n",
    "all_processed_chunks = []\n",
    "\n",
    "for filename, file_path in list(sample_files.items())[:2]:  # Process first 2 files\n",
    "    chunks = process_document_complete(file_path, chunk_strategy=\"sentence\")\n",
    "    all_processed_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\n\udcca Pipeline Summary:\")\n",
    "print(f\"   üìÑ Documents processed: 2\")\n",
    "print(f\"   üìë Total chunks created: {len(all_processed_chunks)}\")\n",
    "print(f\"   üìè Average chunk size: {sum(len(chunk.content) for chunk in all_processed_chunks) / len(all_processed_chunks):.1f} chars\")\n",
    "\n",
    "# Language detection check\n",
    "croatian_chunks = [chunk for chunk in all_processed_chunks\n",
    "                  if any(c in chunk.content for c in 'ƒçƒá≈°≈æƒë')]\n",
    "print(f\"   üá≠üá∑ Chunks with Croatian diacritics: {len(croatian_chunks)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Complete multilingual pipeline working perfectly!\")\n",
    "print(\"üåç Ready for production Croatian document processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "üéØ **Document Processing is Critical**:\n",
    "- Quality processing = better RAG results\n",
    "- Each step (extract‚Üíclean‚Üíchunk) adds value\n",
    "- Bad preprocessing ruins the entire pipeline\n",
    "\n",
    "üá≠üá∑ **Croatian Language Considerations**:\n",
    "- Always preserve diacritics (ƒç, ƒá, ≈°, ≈æ, ƒë)\n",
    "- Handle various encodings (UTF-8, Windows-1250)\n",
    "- Use Croatian-aware sentence splitting\n",
    "- Consider regional variations and dialects\n",
    "\n",
    "üìù **Chunking Strategy Matters**:\n",
    "- Sentence-based: Best for most Croatian content\n",
    "- Paragraph-based: Good for structured documents\n",
    "- Hybrid: Best for complex formats\n",
    "- Always include overlap for context preservation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "‚úÖ **Text Extraction**:\n",
    "- Auto-detect encoding for legacy documents\n",
    "- Preserve file metadata for traceability\n",
    "- Handle extraction errors gracefully\n",
    "\n",
    "‚úÖ **Text Cleaning**:\n",
    "- Use conservative cleaning for Croatian\n",
    "- Never normalize Croatian diacritics\n",
    "- Preserve proper names and technical terms\n",
    "- Normalize whitespace and quotes\n",
    "\n",
    "‚úÖ **Document Chunking**:\n",
    "- Aim for 100-400 characters per chunk\n",
    "- Use 20-50 character overlap\n",
    "- Respect sentence boundaries\n",
    "- Include chunk metadata for debugging\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. ‚úÖ **Document Processing** - Just completed!\n",
    "2. ‚úÖ **Vector Database** - Already done\n",
    "3. ‚è≥ **Retrieval System** - Next step\n",
    "4. ‚è≥ **Generation** - Local LLM integration\n",
    "5. ‚è≥ **Complete Pipeline** - End-to-end integration\n",
    "\n",
    "The document processing pipeline creates clean, well-structured chunks that are ready for embedding and storage in our vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up our temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    if temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"üßπ Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not clean up temp directory: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Document Processing Learning Complete!\")\n",
    "print(\"\\nüìö What we learned:\")\n",
    "print(\"   ‚Ä¢ Text extraction from multiple formats\")\n",
    "print(\"   ‚Ä¢ Croatian-specific cleaning challenges\")\n",
    "print(\"   ‚Ä¢ Chunking strategies and their impact\")\n",
    "print(\"   ‚Ä¢ Complete processing pipeline design\")\n",
    "print(\"\\n‚û°Ô∏è  Ready for the next step: Vector Database integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
