{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing for Croatian RAG System\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook explains the document processing pipeline - the first critical step in building a Croatian RAG system:\n",
    "\n",
    "1. **Why document processing matters for RAG quality**\n",
    "2. **Text extraction from different file formats**\n",
    "3. **Croatian language-specific cleaning challenges**\n",
    "4. **Document chunking strategies and their impact**\n",
    "5. **Building a complete preprocessing pipeline**\n",
    "\n",
    "## 1. Why Document Processing Matters\n",
    "\n",
    "### The Foundation of RAG Quality\n",
    "\n",
    "Document processing is like preparing ingredients for cooking - poor preparation ruins the final dish. In RAG systems:\n",
    "\n",
    "- **Garbage In = Garbage Out**: Bad preprocessing leads to poor retrieval\n",
    "- **Chunk Quality**: How you split documents affects search relevance\n",
    "- **Language-Specific Issues**: Croatian diacritics and encoding must be preserved\n",
    "- **Metadata Preservation**: Source information helps with citations\n",
    "\n",
    "### Croatian Language Challenges\n",
    "\n",
    "Croatian text processing has unique challenges:\n",
    "- **Diacritics**: Ä, Ä‡, Å¡, Å¾, Ä‘ must be preserved correctly\n",
    "- **Encoding Issues**: Many documents use Windows-1250 or ISO-8859-2\n",
    "- **Regional Variations**: Different dialects and spellings\n",
    "- **Mixed Content**: Documents often contain English/German/Italian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our document processing components\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from preprocessing.extractors import DocumentExtractor, TextExtractor, PDFExtractor, DocxExtractor\n",
    "from preprocessing.cleaners import CroatianTextCleaner, TextCleaningConfig\n",
    "from preprocessing.chunkers import DocumentChunker, ChunkingConfig, ChunkingStrategy\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "print(\"âœ… Document processing components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Extraction from Different Formats\n",
    "\n",
    "Our system supports three main document types:\n",
    "\n",
    "### ğŸ“„ Plain Text (.txt)\n",
    "- Simplest format but encoding can be tricky\n",
    "- Croatian documents often use Windows-1250 encoding\n",
    "- UTF-8 detection and conversion is crucial\n",
    "\n",
    "### ğŸ“• PDF Documents (.pdf)\n",
    "- Complex format with fonts, images, layouts\n",
    "- May contain scanned text (OCR needed)\n",
    "- Croatian PDFs may have embedded font issues\n",
    "\n",
    "### ğŸ“˜ Word Documents (.docx)\n",
    "- Structured format with styles, headers\n",
    "- Generally good encoding support\n",
    "- May contain tables, images, footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some sample Croatian documents for testing\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for our test documents\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=\"croatian_docs_\"))\n",
    "print(f\"ğŸ“ Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Sample Croatian texts with various challenges\n",
    "sample_texts = {\n",
    "    \"zagreb_info.txt\": \"\"\"\n",
    "Zagreb - Glavni Grad Hrvatske\n",
    "\n",
    "Zagreb je glavni i najveÄ‡i grad Republike Hrvatske. SmjeÅ¡ten je u sjeverozapadnom dijelu zemlje, na rijeci Savi. \n",
    "Grad ima bogatu povijest koja seÅ¾e u rimsko doba.\n",
    "\n",
    "TURISTIÄŒKA MJESTA:\n",
    "â€¢ Gornji grad - povijesni dio s crkvom sv. Marka\n",
    "â€¢ Donji grad - trgovaÄki i poslovni centar\n",
    "â€¢ Maksimir - najveÄ‡i park u Zagrebu\n",
    "\n",
    "StanovniÅ¡tvo: ~800,000 stanovnika (2021.)\n",
    "PovrÅ¡ina: 641.4 kmÂ²\n",
    "\n",
    "Zagreb je takoÄ‘er kulturno srediÅ¡te Hrvatske s brojnim muzejima, kazaliÅ¡tima i galerijama.\n",
    "\"\"\",\n",
    "    \n",
    "    \"dubrovnik_guide.txt\": \"\"\"\n",
    "Dubrovnik - Biser Jadrana\n",
    "\n",
    "Dubrovnik je grad na jugu Hrvatske, poznat kao \"biser Jadrana\". \n",
    "Stari grad je upisan na UNESCO-ovu listu svjetske baÅ¡tine 1979. godine.\n",
    "\n",
    "ZNAMENITOSTI:\n",
    "- Gradske zidine (duljine 1940m)\n",
    "- Stradun - glavna ulica\n",
    "- KneÅ¾ev dvor\n",
    "- FranjevaÄki samostan\n",
    "\n",
    "Dubrovnik je sluÅ¾io kao lokacija za snimanje serije \"Igra prijestolja\" (Game of Thrones).\n",
    "\n",
    "Klima: mediteranska\n",
    "Broj dana sa suncem: >250 godiÅ¡nje\n",
    "\"\"\",\n",
    "    \n",
    "    \"plitvice_nature.txt\": \"\"\"\n",
    "PlitviÄka jezera - Nacionalni park\n",
    "\n",
    "Nacionalni park PlitviÄka jezera nalazi se u Lici i Kordunu. \n",
    "Park je osnovan 1949. godine i jedan je od najstarijih nacionalnih parkova u jugoistoÄnoj Europi.\n",
    "\n",
    "KARAKTERISTIKE:\n",
    "âœ“ 16 jezera povezanih slapovima\n",
    "âœ“ Ukupna povrÅ¡ina: 296.85 kmÂ²\n",
    "âœ“ UNESCO svjetska baÅ¡tina od 1979.\n",
    "\n",
    "Flora i fauna:\n",
    "- Å ume bukve, jele i smreke\n",
    "- SmeÄ‘i medvjed, vuk, ris\n",
    "- ViÅ¡e od 150 vrsta ptica\n",
    "\n",
    "VAÅ½NO: Park je otvoren tijekom cijele godine, ali zimski period ima ograniÄen pristup.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write sample texts to files\n",
    "sample_files = {}\n",
    "for filename, content in sample_texts.items():\n",
    "    file_path = temp_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content.strip())\n",
    "    sample_files[filename] = file_path\n",
    "    print(f\"ğŸ“ Created: {filename} ({len(content)} chars)\")\n",
    "\n",
    "print(f\"\\nâœ… Created {len(sample_files)} sample Croatian documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test text extraction from our sample documents\n",
    "print(\"ğŸ“‚ DOCUMENT EXTRACTION TESTING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create document extractor\n",
    "extractor = DocumentExtractor()\n",
    "\n",
    "for filename, file_path in sample_files.items():\n",
    "    print(f\"\\nğŸ“„ Processing: {filename}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Extract text and metadata\n",
    "        result = extractor.extract_text(str(file_path))\n",
    "        \n",
    "        print(f\"   âœ… Extraction successful\")\n",
    "        print(f\"   ğŸ“Š Text length: {len(result['content'])} characters\")\n",
    "        print(f\"   ğŸ·ï¸  File type: {result['metadata']['file_type']}\")\n",
    "        print(f\"   ğŸ“… File size: {result['metadata']['file_size']} bytes\")\n",
    "        print(f\"   ğŸ”¤ Encoding: {result['metadata']['encoding']}\")\n",
    "        \n",
    "        # Show first 150 characters\n",
    "        preview = result['content'][:150].replace('\\n', ' ')\n",
    "        print(f\"   ğŸ“– Preview: {preview}...\")\n",
    "        \n",
    "        # Check Croatian diacritics preservation\n",
    "        croatian_chars = ['Ä', 'Ä‡', 'Å¡', 'Å¾', 'Ä‘', 'ÄŒ', 'Ä†', 'Å ', 'Å½', 'Ä']\n",
    "        found_diacritics = [char for char in croatian_chars if char in result['content']]\n",
    "        if found_diacritics:\n",
    "            print(f\"   âœ“ Croatian diacritics preserved: {', '.join(found_diacritics)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Extraction failed: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Observations:\")\n",
    "print(\"   â€¢ UTF-8 encoding preserves all Croatian diacritics\")\n",
    "print(\"   â€¢ Metadata extraction provides useful document information\")\n",
    "print(\"   â€¢ Text structure (headers, lists) is preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more complex document with encoding challenges\n",
    "print(\"ğŸ”¤ ENCODING CHALLENGE TESTING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a document with mixed encoding (simulate real-world scenario)\n",
    "problematic_content = \"\"\"\n",
    "ProblematiÄni tekst s razliÄitim kodiranjima\n",
    "\n",
    "Ovaj dokument sadrÅ¾i:\n",
    "â€¢ Hrvatska slova: ÄÄ‡Å¡Å¾Ä‘ ÄŒÄ†Å Å½Ä\n",
    "â€¢ Specijalne znakove: razni znakovi\n",
    "â€¢ Brojeve: 123,456.78 â‚¬\n",
    "â€¢ Datum: 31.12.2023.\n",
    "\n",
    "Problemi s kodiranjem:\n",
    "- Windows-1250: ÃƒÂ¡ Ã„\n",
    "- ISO-8859-2: Â® Â¼ Â½\n",
    "- Stari sistem: [NEÅ¸ITLJIVO]\n",
    "\n",
    "Napomena: Ovaj tekst testira robusnot sistema.\n",
    "\"\"\"\n",
    "\n",
    "# Save with different encodings to test detection\n",
    "encoding_test_files = {}\n",
    "\n",
    "encodings_to_test = [\n",
    "    ('utf-8', 'UTF-8 (standard)'),\n",
    "    ('windows-1250', 'Windows-1250 (Croatian)'),\n",
    "    ('iso-8859-2', 'ISO-8859-2 (Latin-2)')\n",
    "]\n",
    "\n",
    "for encoding, description in encodings_to_test:\n",
    "    filename = f\"encoding_test_{encoding.replace('-', '_')}.txt\"\n",
    "    file_path = temp_dir / filename\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w', encoding=encoding) as f:\n",
    "            f.write(problematic_content)\n",
    "        encoding_test_files[filename] = (file_path, encoding, description)\n",
    "        print(f\"ğŸ“ Created {filename} with {description}\")\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(f\"âŒ Cannot create {filename} with {encoding}: {e}\")\n",
    "\n",
    "print(\"\\nTesting automatic encoding detection:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for filename, (file_path, original_encoding, description) in encoding_test_files.items():\n",
    "    print(f\"\\nğŸ“„ {filename} (originally {description})\")\n",
    "    \n",
    "    try:\n",
    "        result = extractor.extract_text(str(file_path))\n",
    "        detected_encoding = result['metadata']['encoding']\n",
    "        \n",
    "        print(f\"   ğŸ” Detected encoding: {detected_encoding}\")\n",
    "        print(f\"   âœ… Original encoding: {original_encoding}\")\n",
    "        \n",
    "        # Check if Croatian characters are correctly preserved\n",
    "        if 'ÄÄ‡Å¡Å¾Ä‘' in result['content']:\n",
    "            print(f\"   âœ“ Croatian diacritics correctly preserved\")\n",
    "        else:\n",
    "            print(f\"   âŒ Croatian diacritics lost or corrupted\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Encoding is crucial for Croatian text processing!\")\n",
    "print(\"   â€¢ Always use UTF-8 when possible\")\n",
    "print(\"   â€¢ Auto-detection helps with legacy documents\")\n",
    "print(\"   â€¢ Verify diacritic preservation after extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Croatian Text Cleaning Challenges\n",
    "\n",
    "### Why Clean Text?\n",
    "\n",
    "Raw extracted text often contains:\n",
    "- **Formatting artifacts**: Extra spaces, line breaks\n",
    "- **Non-printable characters**: Control characters, BOM\n",
    "- **Inconsistent whitespace**: Tabs, multiple spaces\n",
    "- **OCR errors**: From scanned documents\n",
    "- **Mixed languages**: Headers/footers in other languages\n",
    "\n",
    "### Croatian-Specific Cleaning:\n",
    "\n",
    "1. **Diacritic Normalization**: Ensure consistent diacritic representation\n",
    "2. **Case Handling**: Proper Croatian title case rules\n",
    "3. **Punctuation**: Handle Croatian-specific quotation marks\n",
    "4. **Number Formats**: Croatian uses comma for decimals (123,45)\n",
    "5. **Date Formats**: DD.MM.YYYY. format common in Croatia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test Croatian text cleaning capabilities\n",
    "print(\"ğŸ§¹ CROATIAN TEXT CLEANING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create text cleaner with Croatian-specific configuration\n",
    "cleaning_config = TextCleaningConfig(\n",
    "    normalize_whitespace=True,\n",
    "    remove_extra_newlines=True,\n",
    "    normalize_diacritics=False,  # Keep Croatian diacritics!\n",
    "    lowercase=False,  # Preserve proper names\n",
    "    remove_punctuation=False,\n",
    "    min_word_length=2,\n",
    "    language='hr'  # Croatian-specific rules\n",
    ")\n",
    "\n",
    "cleaner = CroatianTextCleaner(cleaning_config)\n",
    "\n",
    "print(f\"âš™ï¸ Text cleaner configured for Croatian:\")\n",
    "print(f\"   â€¢ Language: {cleaning_config.language}\")\n",
    "print(f\"   â€¢ Preserve diacritics: {not cleaning_config.normalize_diacritics}\")\n",
    "print(f\"   â€¢ Preserve case: {not cleaning_config.lowercase}\")\n",
    "print(f\"   â€¢ Min word length: {cleaning_config.min_word_length}\")\n",
    "\n",
    "# Test with messy Croatian text\n",
    "messy_text = \"\"\"\n",
    "\n",
    "   ZAGREB    -   GLAVNI GRAD    \n",
    "\n",
    "\n",
    "Zagreb  je  glavni   grad  Republike   Hrvatske.   \n",
    "    Nalazi    se   u   sjeverozapadnom    dijelu    zemlje.  \n",
    "\n",
    "\n",
    "StanovniÅ¡tvo:     ~800,000     stanovnika     (2021.)    \n",
    "\n",
    "PovrÅ¡ina:   641.4    kmÂ²   \n",
    "\n",
    "\n",
    "VAÅ½NE   ÄŒINJENICE:  \n",
    "â€¢    Osnovan    je    u    11.    stoljeÄ‡u  \n",
    "â€¢  Glavni    grad   od    1991.  godine  \n",
    "â€¢    SveuÄiliÅ¡te    osnovano    1669.    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ“ Original messy text:\")\n",
    "print(f\"   Length: {len(messy_text)} characters\")\n",
    "print(f\"   Lines: {messy_text.count(chr(10))} newlines\")\n",
    "print(f\"   Preview: {repr(messy_text[:100])}...\")\n",
    "\n",
    "# Clean the text\n",
    "try:\n",
    "    cleaned_result = cleaner.clean_text(messy_text)\n",
    "    cleaned_text = cleaned_result['text']\n",
    "    \n",
    "    print(\"\\nâœ¨ Cleaned text:\")\n",
    "    print(f\"   Length: {len(cleaned_text)} characters (reduced by {len(messy_text) - len(cleaned_text)})\")\n",
    "    print(f\"   Lines: {cleaned_text.count(chr(10))} newlines\")\n",
    "    print(f\"   Preview: {cleaned_text[:200]}...\")\n",
    "    \n",
    "    # Show cleaning statistics\n",
    "    stats = cleaned_result['metadata']['cleaning_stats']\n",
    "    print(\"\\nğŸ“Š Cleaning Statistics:\")\n",
    "    for operation, count in stats.items():\n",
    "        if count > 0:\n",
    "            print(f\"   â€¢ {operation}: {count}\")\n",
    "            \n",
    "    # Verify Croatian diacritics are preserved\n",
    "    croatian_chars = ['Ä', 'Ä‡', 'Å¡', 'Å¾', 'Ä‘']\n",
    "    found_chars = [char for char in croatian_chars if char in cleaned_text]\n",
    "    if found_chars:\n",
    "        print(f\"\\nâœ“ Croatian diacritics preserved: {', '.join(found_chars)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Cleaning failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different cleaning configurations\n",
    "print(\"ğŸ”§ CLEANING CONFIGURATION COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_text = \"Dubrovnik je POZNAT kao \"biser Jadrana\". Temperatura: 25,5Â°C (77.9Â°F). Datum: 15.08.2023.\"\n",
    "\n",
    "print(f\"ğŸ“ Test text: {test_text}\")\n",
    "print()\n",
    "\n",
    "# Different cleaning strategies\n",
    "configs = [\n",
    "    (\"Conservative\", {\n",
    "        'normalize_diacritics': False,\n",
    "        'lowercase': False,\n",
    "        'remove_punctuation': False,\n",
    "        'normalize_quotes': True\n",
    "    }),\n",
    "    (\"Aggressive\", {\n",
    "        'normalize_diacritics': True,\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'normalize_quotes': True\n",
    "    }),\n",
    "    (\"Balanced\", {\n",
    "        'normalize_diacritics': False,\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': False,\n",
    "        'normalize_quotes': True\n",
    "    })\n",
    "]\n",
    "\n",
    "for strategy_name, config_updates in configs:\n",
    "    print(f\"ğŸ› ï¸ {strategy_name} Cleaning:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create config with updates\n",
    "    config = TextCleaningConfig(\n",
    "        language='hr',\n",
    "        **config_updates\n",
    "    )\n",
    "    \n",
    "    cleaner_test = CroatianTextCleaner(config)\n",
    "    \n",
    "    try:\n",
    "        result = cleaner_test.clean_text(test_text)\n",
    "        cleaned = result['text']\n",
    "        \n",
    "        print(f\"   Input:  {test_text}\")\n",
    "        print(f\"   Output: {cleaned}\")\n",
    "        \n",
    "        # Analysis\n",
    "        changes = []\n",
    "        if config.normalize_diacritics and any(c in test_text for c in 'ÄÄ‡Å¡Å¾Ä‘'):\n",
    "            changes.append(\"diacritics normalized\")\n",
    "        if config.lowercase and any(c.isupper() for c in test_text):\n",
    "            changes.append(\"lowercased\")\n",
    "        if config.remove_punctuation:\n",
    "            changes.append(\"punctuation removed\")\n",
    "        if config.normalize_quotes and \"smart quotes\" in test_text:\n",
    "            changes.append(\"quotes normalized\")\n",
    "            \n",
    "        if changes:\n",
    "            print(f\"   Changes: {', '.join(changes)}\")\n",
    "        else:\n",
    "            print(f\"   Changes: none\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ Cleaning Strategy Recommendations:\")\n",
    "print(\"   â€¢ Conservative: Best for formal documents, citations\")\n",
    "print(\"   â€¢ Aggressive: Good for search indexing, analysis\")\n",
    "print(\"   â€¢ Balanced: Good compromise for most RAG applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Chunking Strategies\n",
    "\n",
    "### Why Chunk Documents?\n",
    "\n",
    "Large documents must be split into smaller pieces because:\n",
    "- **Embedding limits**: Models have max token limits (512-1024 tokens)\n",
    "- **Search precision**: Smaller chunks = more focused results\n",
    "- **Context relevance**: Large chunks may contain irrelevant information\n",
    "- **Processing efficiency**: Smaller pieces are faster to process\n",
    "\n",
    "### Chunking Strategies:\n",
    "\n",
    "1. **Sentence-based**: Split on sentence boundaries (good for Croatian)\n",
    "2. **Fixed-size**: Split by character/token count\n",
    "3. **Paragraph-based**: Split on paragraph breaks\n",
    "4. **Semantic**: Split based on topic/meaning\n",
    "5. **Hybrid**: Combine multiple approaches\n",
    "\n",
    "### Croatian Considerations:\n",
    "\n",
    "- **Sentence detection**: Croatian punctuation patterns\n",
    "- **Long sentences**: Croatian can have very long complex sentences\n",
    "- **Paragraph structure**: Formal vs informal text differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test different chunking strategies on Croatian text\n",
    "print(\"ğŸ“ DOCUMENT CHUNKING STRATEGIES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use one of our cleaned documents as test content\n",
    "test_document = \"\"\"\n",
    "Zagreb - Glavni Grad Hrvatske\n",
    "\n",
    "Zagreb je glavni i najveÄ‡i grad Republike Hrvatske. SmjeÅ¡ten je u sjeverozapadnom dijelu zemlje, na rijeci Savi. Grad ima bogatu povijest koja seÅ¾e u rimsko doba.\n",
    "\n",
    "Povijesni razvoj grada moÅ¾emo pratiti kroz nekoliko vaÅ¾nih razdoblja. U rimsko doba na ovom je prostoru postojao grad Andautonia. Srednjovjekovni Zagreb nastao je spajaanjem dvaju gradova: Kaptola i Gradeca.\n",
    "\n",
    "TuristiÄka mjesta u Zagrebu ukljuÄuju Gornji grad - povijesni dio s crkvom sv. Marka, Donji grad - trgovaÄki i poslovni centar, te Maksimir - najveÄ‡i park u Zagrebu. Grad je takoÄ‘er poznato kulturno srediÅ¡te s brojnim muzejima, kazaliÅ¡tima i galerijama.\n",
    "\n",
    "Zagreb je dom mnogih vaÅ¾nih institucija. Tu se nalaze sveuÄiliÅ¡te, akademija znanosti, nacionalna knjiÅ¾nica i mnoge druge ustanove. Grad je takoÄ‘er vaÅ¾no gospodarsko srediÅ¡te regije.\n",
    "\n",
    "StanovniÅ¡tvo Zagreba broji oko 800.000 stanovnika, Å¡to ga Äini najveÄ‡im gradom u Hrvatskoj. PovrÅ¡ina grada iznosi 641.4 kmÂ². Klima je umjereno kontinentalna s toplim ljetima i hladnim zimama.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(f\"ğŸ“„ Test document:\")\n",
    "print(f\"   Length: {len(test_document)} characters\")\n",
    "print(f\"   Paragraphs: {test_document.count(chr(10) + chr(10)) + 1}\")\n",
    "print(f\"   Sentences: ~{test_document.count('.')}\")\n",
    "print()\n",
    "\n",
    "# Test different chunking strategies\n",
    "chunking_strategies = [\n",
    "    (\"Sentence-based\", ChunkingStrategy.SENTENCE, {\"max_chunk_size\": 200, \"overlap_size\": 20}),\n",
    "    (\"Fixed-size\", ChunkingStrategy.FIXED_SIZE, {\"max_chunk_size\": 150, \"overlap_size\": 30}),\n",
    "    (\"Paragraph-based\", ChunkingStrategy.PARAGRAPH, {\"max_chunk_size\": 400, \"overlap_size\": 50}),\n",
    "    (\"Hybrid\", ChunkingStrategy.HYBRID, {\"max_chunk_size\": 250, \"overlap_size\": 40})\n",
    "]\n",
    "\n",
    "for strategy_name, strategy, params in chunking_strategies:\n",
    "    print(f\"ğŸ“‘ {strategy_name} Chunking:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Configure chunker\n",
    "        config = ChunkingConfig(\n",
    "            strategy=strategy,\n",
    "            max_chunk_size=params[\"max_chunk_size\"],\n",
    "            overlap_size=params[\"overlap_size\"],\n",
    "            language='hr'\n",
    "        )\n",
    "        \n",
    "        chunker = DocumentChunker(config)\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = chunker.chunk_text(\n",
    "            text=test_document,\n",
    "            metadata={\n",
    "                \"source\": \"zagreb_info.txt\",\n",
    "                \"title\": \"Zagreb - Glavni Grad Hrvatske\",\n",
    "                \"language\": \"hr\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Created {len(chunks)} chunks\")\n",
    "        print(f\"   ğŸ“Š Config: max_size={params['max_chunk_size']}, overlap={params['overlap_size']}\")\n",
    "        \n",
    "        # Analyze chunk sizes\n",
    "        chunk_sizes = [len(chunk['content']) for chunk in chunks]\n",
    "        avg_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0\n",
    "        print(f\"   ğŸ“ Sizes: avg={avg_size:.1f}, min={min(chunk_sizes)}, max={max(chunk_sizes)}\")\n",
    "        \n",
    "        # Show first few chunks\n",
    "        for i, chunk in enumerate(chunks[:2]):\n",
    "            preview = chunk['content'][:80].replace('\\n', ' ')\n",
    "            print(f\"   ğŸ“ Chunk {i+1}: {preview}...\")\n",
    "            \n",
    "        if len(chunks) > 2:\n",
    "            print(f\"   â‹¯ ({len(chunks) - 2} more chunks)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Chunking failed: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunking quality for Croatian text\n",
    "print(\"ğŸ” CHUNKING QUALITY ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with sentence-based chunking (good for Croatian)\n",
    "config = ChunkingConfig(\n",
    "    strategy=ChunkingStrategy.SENTENCE,\n",
    "    max_chunk_size=300,\n",
    "    overlap_size=50,\n",
    "    language='hr'\n",
    ")\n",
    "\n",
    "chunker = DocumentChunker(config)\n",
    "\n",
    "try:\n",
    "    chunks = chunker.chunk_text(\n",
    "        text=test_document,\n",
    "        metadata={\n",
    "            \"source\": \"zagreb_analysis.txt\",\n",
    "            \"title\": \"Zagreb Analysis\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Quality Analysis for {len(chunks)} chunks:\")\n",
    "    print()\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content = chunk['content']\n",
    "        metadata = chunk['metadata']\n",
    "        \n",
    "        # Analyze chunk quality\n",
    "        sentences = content.count('. ') + content.count('!') + content.count('?')\n",
    "        words = len(content.split())\n",
    "        has_header = any(line.isupper() or line.startswith('#') for line in content.split('\\n'))\n",
    "        \n",
    "        print(f\"ğŸ“„ Chunk {i+1} (ID: {metadata['chunk_id'][:8]}...):\")\n",
    "        print(f\"   ğŸ“ Length: {len(content)} chars, {words} words, ~{sentences} sentences\")\n",
    "        print(f\"   ğŸ”¢ Index: {metadata['chunk_index']}/{metadata['total_chunks']}\")\n",
    "        print(f\"   ğŸ·ï¸  Header: {'Yes' if has_header else 'No'}\")\n",
    "        \n",
    "        # Show content with line breaks preserved\n",
    "        lines = content.split('\\n')[:3]  # First 3 lines\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                preview = line[:60] + \"...\" if len(line) > 60 else line\n",
    "                print(f\"   ğŸ“ {preview}\")\n",
    "        \n",
    "        if len(content.split('\\n')) > 3:\n",
    "            print(f\"   â‹¯ ({len(content.split('\\n')) - 3} more lines)\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    # Calculate overlap analysis\n",
    "    if len(chunks) > 1:\n",
    "        print(\"ğŸ”— Overlap Analysis:\")\n",
    "        for i in range(len(chunks) - 1):\n",
    "            current_end = chunks[i]['content'][-50:]\n",
    "            next_start = chunks[i+1]['content'][:50]\n",
    "            \n",
    "            # Simple overlap detection\n",
    "            overlap_words = set(current_end.split()) & set(next_start.split())\n",
    "            print(f\"   Chunk {i+1}â†’{i+2}: {len(overlap_words)} overlapping words\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Analysis failed: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Good chunks should:\")\n",
    "print(\"   â€¢ Be complete thoughts (end at sentence boundaries)\")\n",
    "print(\"   â€¢ Have reasonable size (100-500 characters for Croatian)\")\n",
    "print(\"   â€¢ Preserve context through overlap\")\n",
    "print(\"   â€¢ Maintain structural information (headers, lists)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking with different types of Croatian content\n",
    "print(\"ğŸ“š CHUNKING DIFFERENT CONTENT TYPES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Different Croatian content types\n",
    "content_types = {\n",
    "    \"News Article\": \"\"\"\n",
    "ZAGREB, 15. srpnja 2023. - Hrvatski premijer najavio je nova ulaganja u infrastrukturu. \n",
    "\n",
    "Prema rijeÄima premijera, projekti Ä‡e ukljuÄivati modernizaciju cesta, Å¾eljeznica i digitalne infrastrukture. \"Ovo su strateÅ¡ka ulaganja za buduÄ‡nost Hrvatske\", rekao je premijer na tiskovnoj konferenciji.\n",
    "\n",
    "Ukupna vrijednost projekata procjenjuje se na 2,5 milijardi eura. Financiranje Ä‡e biti osigurano iz EU fondova i drÅ¾avnog proraÄuna.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Academic Text\": \"\"\"\n",
    "Sintaksa hrvatskog jezika odlikuje se sloÅ¾enoÅ¡Ä‡u koja proizlazi iz bogate morfologije. Fleksijska priroda jezika omoguÄ‡uje relativno slobodan red rijeÄi, Å¡to se posebno oÄituje u poetskom diskursu.\n",
    "\n",
    "Slavenski jezici, ukljuÄujuÄ‡i hrvatski, karakterizira razvijen aspektualnost glagolski sistem. Perfektivnost i imperfektivnost glagola fundamentalne su kategorije koje utjeÄu na temporalnu strukturu iskaza.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Recipe\": \"\"\"\n",
    "SARMA - tradicionalno zimsko jelo\n",
    "\n",
    "Potrebno:\n",
    "â€¢ 1 kg mijeÅ¡anog mesa\n",
    "â€¢ 1 glavica kiselog kupusa\n",
    "â€¢ 200g riÅ¾e\n",
    "â€¢ 2 luka\n",
    "â€¢ Sol, papar, vegeta\n",
    "\n",
    "Priprema:\n",
    "1. Prokuhajte riÅ¾u na pola\n",
    "2. PomijeÅ¡ajte meso s riÅ¾om i zaÄinima\n",
    "3. Zamotajte u kupusove listove\n",
    "4. Kuhajte 2 sata na laganoj vatri\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for content_name, content in content_types.items():\n",
    "    print(f\"ğŸ“– {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "    \n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "    \n",
    "    chunker = DocumentChunker(config)\n",
    "    \n",
    "    try:\n",
    "        chunks = chunker.chunk_text(\n",
    "            text=content.strip(),\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"   ğŸ“Š Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   ğŸ“‘ Chunks created: {len(chunks)}\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"ğŸ¯ Content-specific chunking improves RAG quality:\")\n",
    "print(\"   â€¢ News: Paragraph-based preserves story flow\")\n",
    "print(\"   â€¢ Academic: Sentence-based handles complexity\")\n",
    "print(\"   â€¢ Recipes: Hybrid preserves structured lists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Document Processing Pipeline\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "A complete document processing pipeline combines:\n",
    "1. **Document detection and loading**\n",
    "2. **Format-specific text extraction**\n",
    "3. **Croatian-aware text cleaning**\n",
    "4. **Intelligent document chunking**\n",
    "5. **Metadata preservation and enrichment**\n",
    "\n",
    "### Pipeline Benefits:\n",
    "- **Consistency**: Same processing for all documents\n",
    "- **Quality**: Each step improves text quality\n",
    "- **Scalability**: Can process hundreds of documents\n",
    "- **Traceability**: Track processing steps and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a complete document processing pipeline\n",
    "print(\"ğŸ”„ COMPLETE DOCUMENT PROCESSING PIPELINE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def process_document_pipeline(file_path: str, chunk_strategy: str = \"sentence\") -> list:\n",
    "    \"\"\"\n",
    "    Complete pipeline to process a Croatian document from file to chunks.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"ğŸ“‚ Processing: {Path(file_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        print(\"   1ï¸âƒ£ Extracting text...\")\n",
    "        extractor = DocumentExtractor()\n",
    "        extraction_result = extractor.extract_text(file_path)\n",
    "        \n",
    "        raw_text = extraction_result['content']\n",
    "        file_metadata = extraction_result['metadata']\n",
    "        \n",
    "        print(f\"      âœ… Extracted {len(raw_text)} characters\")\n",
    "        print(f\"      ğŸ“„ File type: {file_metadata['file_type']}\")\n",
    "        print(f\"      ğŸ”¤ Encoding: {file_metadata['encoding']}\")\n",
    "        \n",
    "        # Step 2: Clean text\n",
    "        print(\"   2ï¸âƒ£ Cleaning Croatian text...\")\n",
    "        cleaning_config = TextCleaningConfig(\n",
    "            language='hr',\n",
    "            normalize_whitespace=True,\n",
    "            remove_extra_newlines=True,\n",
    "            normalize_diacritics=False,  # Preserve Croatian characters\n",
    "            normalize_quotes=True\n",
    "        )\n",
    "        \n",
    "        cleaner = CroatianTextCleaner(cleaning_config)\n",
    "        cleaning_result = cleaner.clean_text(raw_text)\n",
    "        \n",
    "        cleaned_text = cleaning_result['text']\n",
    "        cleaning_stats = cleaning_result['metadata']['cleaning_stats']\n",
    "        \n",
    "        print(f\"      âœ… Cleaned to {len(cleaned_text)} characters\")\n",
    "        print(f\"      ğŸ§¹ Cleaning operations: {sum(cleaning_stats.values())}\")\n",
    "        \n",
    "        # Step 3: Create chunks\n",
    "        print(f\"   3ï¸âƒ£ Creating chunks ({chunk_strategy} strategy)...\")\n",
    "        \n",
    "        strategy_map = {\n",
    "            \"sentence\": ChunkingStrategy.SENTENCE,\n",
    "            \"paragraph\": ChunkingStrategy.PARAGRAPH,\n",
    "            \"hybrid\": ChunkingStrategy.HYBRID\n",
    "        }\n",
    "        \n",
    "        chunking_config = ChunkingConfig(\n",
    "            strategy=strategy_map.get(chunk_strategy, ChunkingStrategy.SENTENCE),\n",
    "            max_chunk_size=300,\n",
    "            overlap_size=50,\n",
    "            language='hr'\n",
    "        )\n",
    "        \n",
    "        chunker = DocumentChunker(chunking_config)\n",
    "        \n",
    "        # Combine metadata\n",
    "        combined_metadata = {\n",
    "            **file_metadata,\n",
    "            'cleaning_stats': cleaning_stats,\n",
    "            'chunking_strategy': chunk_strategy\n",
    "        }\n",
    "        \n",
    "        chunks = chunker.chunk_text(cleaned_text, combined_metadata)\n",
    "        \n",
    "        print(f\"      âœ… Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 4: Quality check\n",
    "        print(\"   4ï¸âƒ£ Quality check...\")\n",
    "        \n",
    "        chunk_sizes = [len(chunk['content']) for chunk in chunks]\n",
    "        avg_size = sum(chunk_sizes) / len(chunk_sizes)\n",
    "        \n",
    "        quality_issues = []\n",
    "        if avg_size < 50:\n",
    "            quality_issues.append(\"chunks too small\")\n",
    "        if max(chunk_sizes) > 500:\n",
    "            quality_issues.append(\"some chunks too large\")\n",
    "        if not any('Ä' in chunk['content'] or 'Ä‡' in chunk['content'] or \n",
    "                  'Å¡' in chunk['content'] or 'Å¾' in chunk['content'] or \n",
    "                  'Ä‘' in chunk['content'] for chunk in chunks):\n",
    "            if any(c in raw_text for c in 'ÄÄ‡Å¡Å¾Ä‘'):\n",
    "                quality_issues.append(\"Croatian diacritics lost\")\n",
    "        \n",
    "        if quality_issues:\n",
    "            print(f\"      âš ï¸ Quality issues: {', '.join(quality_issues)}\")\n",
    "        else:\n",
    "            print(f\"      âœ… Quality check passed\")\n",
    "        \n",
    "        print(f\"      ğŸ“Š Avg chunk size: {avg_size:.1f} chars\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Pipeline failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test the complete pipeline\n",
    "print(\"ğŸš€ Testing complete pipeline on our sample documents:\")\n",
    "print()\n",
    "\n",
    "all_processed_chunks = []\n",
    "\n",
    "for filename, file_path in list(sample_files.items())[:2]:  # Process first 2 files\n",
    "    chunks = process_document_pipeline(str(file_path), \"sentence\")\n",
    "    all_processed_chunks.extend(chunks)\n",
    "    print()\n",
    "\n",
    "print(f\"ğŸ“ˆ Pipeline Summary:\")\n",
    "print(f\"   ğŸ“ Documents processed: {len(sample_files)}\")\n",
    "print(f\"   ğŸ“„ Total chunks created: {len(all_processed_chunks)}\")\n",
    "print(f\"   ğŸ’¾ Ready for vector database storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the final processed chunks\n",
    "if all_processed_chunks:\n",
    "    print(\"ğŸ” FINAL CHUNK ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"ğŸ“Š Chunk Statistics:\")\n",
    "    chunk_lengths = [len(chunk['content']) for chunk in all_processed_chunks]\n",
    "    print(f\"   â€¢ Total chunks: {len(all_processed_chunks)}\")\n",
    "    print(f\"   â€¢ Average length: {sum(chunk_lengths) / len(chunk_lengths):.1f} characters\")\n",
    "    print(f\"   â€¢ Min length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"   â€¢ Max length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Sample some chunks\n",
    "    print(\"\\nğŸ“ Sample Processed Chunks:\")\n",
    "    for i, chunk in enumerate(all_processed_chunks[:3]):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"   ğŸ“ Length: {len(chunk['content'])} chars\")\n",
    "        print(f\"   ğŸ·ï¸  Source: {chunk['metadata']['source']}\")\n",
    "        print(f\"   ğŸ”¢ Index: {chunk['metadata']['chunk_index']}/{chunk['metadata']['total_chunks']}\")\n",
    "        \n",
    "        # Show content\n",
    "        content_lines = chunk['content'].split('\\n')[:2]\n",
    "        for line in content_lines:\n",
    "            if line.strip():\n",
    "                print(f\"   ğŸ“„ {line[:70]}...\" if len(line) > 70 else f\"   ğŸ“„ {line}\")\n",
    "        \n",
    "        # Check Croatian preservation\n",
    "        croatian_chars = [c for c in 'ÄÄ‡Å¡Å¾Ä‘ÄŒÄ†Å Å½Ä' if c in chunk['content']]\n",
    "        if croatian_chars:\n",
    "            print(f\"   âœ“ Croatian chars: {', '.join(set(croatian_chars))}\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ These chunks are now ready for:\")\n",
    "    print(\"   1. Embedding generation (vector database)\")\n",
    "    print(\"   2. Metadata indexing\")\n",
    "    print(\"   3. Similarity search\")\n",
    "    print(\"   4. RAG retrieval\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸  No processed chunks available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "ğŸ¯ **Document Processing is Critical**:\n",
    "- Quality processing = better RAG results\n",
    "- Each step (extractâ†’cleanâ†’chunk) adds value\n",
    "- Bad preprocessing ruins the entire pipeline\n",
    "\n",
    "ğŸ‡­ğŸ‡· **Croatian Language Considerations**:\n",
    "- Always preserve diacritics (Ä, Ä‡, Å¡, Å¾, Ä‘)\n",
    "- Handle various encodings (UTF-8, Windows-1250)\n",
    "- Use Croatian-aware sentence splitting\n",
    "- Consider regional variations and dialects\n",
    "\n",
    "ğŸ“ **Chunking Strategy Matters**:\n",
    "- Sentence-based: Best for most Croatian content\n",
    "- Paragraph-based: Good for structured documents\n",
    "- Hybrid: Best for complex formats\n",
    "- Always include overlap for context preservation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "âœ… **Text Extraction**:\n",
    "- Auto-detect encoding for legacy documents\n",
    "- Preserve file metadata for traceability\n",
    "- Handle extraction errors gracefully\n",
    "\n",
    "âœ… **Text Cleaning**:\n",
    "- Use conservative cleaning for Croatian\n",
    "- Never normalize Croatian diacritics\n",
    "- Preserve proper names and technical terms\n",
    "- Normalize whitespace and quotes\n",
    "\n",
    "âœ… **Document Chunking**:\n",
    "- Aim for 100-400 characters per chunk\n",
    "- Use 20-50 character overlap\n",
    "- Respect sentence boundaries\n",
    "- Include chunk metadata for debugging\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. âœ… **Document Processing** - Just completed!\n",
    "2. âœ… **Vector Database** - Already done\n",
    "3. â³ **Retrieval System** - Next step\n",
    "4. â³ **Generation** - Local LLM integration\n",
    "5. â³ **Complete Pipeline** - End-to-end integration\n",
    "\n",
    "The document processing pipeline creates clean, well-structured chunks that are ready for embedding and storage in our vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up our temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    if temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"ğŸ§¹ Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Warning: Could not clean up temp directory: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Document Processing Learning Complete!\")\n",
    "print(\"\\nğŸ“š What we learned:\")\n",
    "print(\"   â€¢ Text extraction from multiple formats\")\n",
    "print(\"   â€¢ Croatian-specific cleaning challenges\")\n",
    "print(\"   â€¢ Chunking strategies and their impact\")\n",
    "print(\"   â€¢ Complete processing pipeline design\")\n",
    "print(\"\\nâ¡ï¸  Ready for the next step: Vector Database integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
