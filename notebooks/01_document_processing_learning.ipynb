{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Document Processing for RAG System\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook explains the multilingual document processing pipeline - the first critical step in building a scalable multilingual RAG system:\n",
    "\n",
    "1. **Why language-aware document processing matters for RAG quality**\n",
    "2. **Text extraction from different file formats across languages**\n",
    "3. **Language-specific cleaning challenges (Croatian, English, extensible)**\n",
    "4. **Language-aware document chunking strategies and their impact**\n",
    "5. **Building a complete multilingual preprocessing pipeline**\n",
    "6. **Automatic language detection and folder organization**\n",
    "\n",
    "## 1. Why Multilingual Document Processing Matters\n",
    "\n",
    "### The Foundation of Cross-Language RAG Quality\n",
    "\n",
    "Document processing is like preparing ingredients for international cuisine - language-specific preparation ensures authentic results. In multilingual RAG systems:\n",
    "\n",
    "- **Language Preservation**: Maintain language-specific features (diacritics, scripts, etc.)\n",
    "- **Cross-Language Consistency**: Unified processing while respecting language differences\n",
    "- **Chunk Quality**: Language-aware splitting for better semantic coherence\n",
    "- **Metadata Enhancement**: Language tags and source information for better routing\n",
    "- **Encoding Robustness**: Handle various character encodings across languages\n",
    "\n",
    "### Language-Specific Processing Challenges\n",
    "\n",
    "#### 🇭🇷 Croatian Language Challenges\n",
    "- **Diacritics**: č, ć, š, ž, đ must be preserved correctly\n",
    "- **Encoding Issues**: Many documents use Windows-1250 or ISO-8859-2\n",
    "- **Morphological Complexity**: Rich inflectional system\n",
    "- **Regional Variations**: Different dialects and spellings\n",
    "\n",
    "#### 🇬🇧 English Language Challenges  \n",
    "- **Business Documents**: Financial reports, legal contracts, technical specs\n",
    "- **Encoding Variations**: UTF-8, cp1252, iso-8859-1\n",
    "- **Technical Terminology**: Industry-specific vocabulary preservation\n",
    "- **Document Structure**: Complex layouts in business documents\n",
    "\n",
    "#### 🌐 Cross-Language Challenges\n",
    "- **Language Detection**: Automatic identification for proper routing\n",
    "- **Mixed Documents**: Handling multilingual content within single documents\n",
    "- **Unified Metadata**: Consistent tagging across language boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our multilingual document processing components\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from src.preprocessing.extractors import DocumentExtractor\n",
    "from src.preprocessing.cleaners import MultilingualTextCleaner\n",
    "from src.preprocessing.chunkers import DocumentChunker, chunk_document\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "print(\"✅ Multilingual document processing components imported successfully!\")\n",
    "print(\"🌍 Supporting Croatian, English, and extensible language framework\")\n",
    "print(\"📁 Language-aware processing with automatic detection capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilingual Document Organization\n",
    "\n",
    "### 📁 Language-Based Folder Structure\n",
    "\n",
    "Our system uses a language-aware folder organization for scalable document management:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── raw/\n",
    "│   ├── hr/                    # Croatian documents\n",
    "│   │   ├── NN-2025-115-1666.pdf\n",
    "│   │   └── legal_document.docx\n",
    "│   ├── en/                    # English documents\n",
    "│   │   ├── financial_report.pdf\n",
    "│   │   └── business_plan.docx\n",
    "│   └── multilingual/          # Mixed-language documents\n",
    "├── processed/\n",
    "│   ├── hr/                    # Croatian processed chunks\n",
    "│   ├── en/                    # English processed chunks\n",
    "│   └── shared/                # Cross-language resources\n",
    "└── test/\n",
    "    ├── hr/sample_croatian.txt\n",
    "    ├── en/sample_english.txt\n",
    "    └── multilingual/mixed_content.txt\n",
    "```\n",
    "\n",
    "### 🔄 Language Detection and Routing\n",
    "\n",
    "1. **Automatic Detection**: Identify document language during ingestion\n",
    "2. **Folder Routing**: Move documents to appropriate language folders\n",
    "3. **Language-Specific Processing**: Apply language-aware cleaning and chunking\n",
    "4. **Unified Storage**: Store in multilingual vector database with language metadata\n",
    "\n",
    "## 3. Text Extraction from Different Formats\n",
    "\n",
    "Our system supports three main document types across all languages:\n",
    "\n",
    "### 📄 Plain Text (.txt)\n",
    "- Simplest format but encoding can be tricky\n",
    "- **Croatian**: Often use Windows-1250 encoding, diacritics preservation\n",
    "- **English**: Usually UTF-8, but may encounter cp1252, iso-8859-1\n",
    "- **Detection**: UTF-8 detection and conversion is crucial\n",
    "\n",
    "### 📕 PDF Documents (.pdf)\n",
    "- Complex format with fonts, images, layouts\n",
    "- May contain scanned text (OCR needed)\n",
    "- **Croatian**: Embedded font issues with diacritics\n",
    "- **English**: Business documents with complex layouts\n",
    "- **Multilingual**: Mixed-language content within single documents\n",
    "\n",
    "### 📘 Word Documents (.docx)\n",
    "- Structured format with styles, headers\n",
    "- Generally good encoding support across languages\n",
    "- May contain tables, images, footnotes\n",
    "- **Language Detection**: Can identify primary language from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create sample multilingual documents for testing\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for our test documents\n",
    "temp_dir = Path(tempfile.mkdtemp(prefix=\"multilingual_docs_\"))\n",
    "print(f\"📁 Created temporary directory: {temp_dir}\")\n",
    "\n",
    "# Create language-specific subdirectories\n",
    "hr_dir = temp_dir / \"hr\"\n",
    "en_dir = temp_dir / \"en\"\n",
    "multilingual_dir = temp_dir / \"multilingual\"\n",
    "\n",
    "for lang_dir in [hr_dir, en_dir, multilingual_dir]:\n",
    "    lang_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Created language directories: hr/, en/, multilingual/\")\n",
    "\n",
    "# Sample Croatian texts with various challenges\n",
    "croatian_texts = {\n",
    "    \"zagreb_info.txt\": \"\"\"\n",
    "Zagreb - Glavni Grad Hrvatske\n",
    "\n",
    "Zagreb je glavni i najveći grad Republike Hrvatske. Smješten je u sjeverozapadnom dijelu zemlje, na rijeci Savi.\n",
    "Grad ima bogatu povijest koja seže u rimsko doba.\n",
    "\n",
    "TURISTIČKA MJESTA:\n",
    "• Gornji grad - povijesni dio s crkvom sv. Marka\n",
    "• Donji grad - trgovački i poslovni centar\n",
    "• Maksimir - najveći park u Zagrebu\n",
    "\n",
    "Stanovništvo: ~800,000 stanovnika (2021.)\n",
    "Površina: 641.4 km²\n",
    "\n",
    "Zagreb je također kulturno središte Hrvatske s brojnim muzejima, kazalištima i galerijama.\n",
    "\"\"\",\n",
    "\n",
    "    \"financije_hr.txt\": \"\"\"\n",
    "IZVJEŠTAJ O FINANCIJSKIM REZULTATIMA\n",
    "\n",
    "Ukupni prihodi: 2.450.000,00 EUR\n",
    "Ukupni troškovi: 1.890.000,00 EUR\n",
    "DOBIT: 560.000,00 EUR\n",
    "\n",
    "Ključni pokazatelji:\n",
    "- Profitabilnost: 22.86%\n",
    "- ROI: 15.3%\n",
    "- Rast prihoda: +8.5% (godina)\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Sample English texts\n",
    "english_texts = {\n",
    "    \"business_report.txt\": \"\"\"\n",
    "QUARTERLY FINANCIAL REPORT - Q3 2025\n",
    "\n",
    "Executive Summary\n",
    "Our company achieved significant growth in Q3 2025, with total revenue reaching €3,250,000.\n",
    "\n",
    "Key Performance Indicators:\n",
    "• Revenue: €3,250,000 (+12.5% YoY)\n",
    "• Operating Profit: €785,000 (+18.2% YoY)\n",
    "• Net Margin: 24.15%\n",
    "• Customer Acquisition: 1,847 new clients\n",
    "\n",
    "Market Analysis:\n",
    "The European market showed strong demand for our services, particularly in the technology and financial sectors.\n",
    "\n",
    "Outlook:\n",
    "We expect continued growth in Q4 2025, projecting revenues of €3.8M.\n",
    "\"\"\",\n",
    "\n",
    "    \"technical_specs.txt\": \"\"\"\n",
    "SYSTEM SPECIFICATIONS\n",
    "\n",
    "Hardware Requirements:\n",
    "- CPU: Intel i7-12700K or AMD Ryzen 7 5800X\n",
    "- RAM: 32GB DDR4-3200\n",
    "- Storage: 1TB NVMe SSD\n",
    "- GPU: NVIDIA RTX 4070 (optional, for acceleration)\n",
    "\n",
    "Software Dependencies:\n",
    "- Python 3.11+\n",
    "- PyTorch 2.0+\n",
    "- Transformers 4.30+\n",
    "- ChromaDB 0.4+\n",
    "\n",
    "Performance Benchmarks:\n",
    "- Query Processing: <150ms\n",
    "- Document Ingestion: 2.5 docs/sec\n",
    "- Embedding Generation: 850 tokens/sec\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Mixed language document\n",
    "multilingual_texts = {\n",
    "    \"mixed_content.txt\": \"\"\"\n",
    "MULTILINGUAL DOCUMENT EXAMPLE\n",
    "\n",
    "English Section:\n",
    "This document demonstrates processing of mixed-language content.\n",
    "Total investment value: €1,500,000\n",
    "\n",
    "Croatian Section:\n",
    "Ovaj dokument prikazuje obradu sadržaja na više jezika.\n",
    "Ukupna vrijednost investicije: 1.500.000,00 EUR\n",
    "\n",
    "Technical Details:\n",
    "- Processing language: Auto-detect\n",
    "- Encoding: UTF-8\n",
    "- Fallback: Language-specific processing\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write Croatian files\n",
    "for filename, content in croatian_texts.items():\n",
    "    file_path = hr_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"🇭🇷 Created Croatian file: {filename}\")\n",
    "\n",
    "# Write English files\n",
    "for filename, content in english_texts.items():\n",
    "    file_path = en_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"🇬🇧 Created English file: {filename}\")\n",
    "\n",
    "# Write multilingual files\n",
    "for filename, content in multilingual_texts.items():\n",
    "    file_path = multilingual_dir / filename\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"🌐 Created multilingual file: {filename}\")\n",
    "\n",
    "print(f\"\\n✅ Created {len(croatian_texts)} Croatian, {len(english_texts)} English, and {len(multilingual_texts)} multilingual test documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test text extraction from our sample documents\n",
    "print(\"📂 DOCUMENT EXTRACTION TESTING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create document extractor\n",
    "extractor = DocumentExtractor()\n",
    "\n",
    "for filename, file_path in sample_files.items():\n",
    "    print(f\"\\n📄 Processing: {filename}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        # Extract text and metadata\n",
    "        result = extractor.extract_text(str(file_path))\n",
    "\n",
    "        print(f\"   ✅ Extraction successful\")\n",
    "        print(f\"   📊 Text length: {len(result['content'])} characters\")\n",
    "        print(f\"   🏷️  File type: {result['metadata']['file_type']}\")\n",
    "        print(f\"   📅 File size: {result['metadata']['file_size']} bytes\")\n",
    "        print(f\"   🔤 Encoding: {result['metadata']['encoding']}\")\n",
    "\n",
    "        # Show first 150 characters\n",
    "        preview = result['content'][:150].replace('\\n', ' ')\n",
    "        print(f\"   📖 Preview: {preview}...\")\n",
    "\n",
    "        # Check Croatian diacritics preservation\n",
    "        croatian_chars = ['č', 'ć', 'š', 'ž', 'đ', 'Č', 'Ć', 'Š', 'Ž', 'Đ']\n",
    "        found_diacritics = [char for char in croatian_chars if char in result['content']]\n",
    "        if found_diacritics:\n",
    "            print(f\"   ✓ Croatian diacritics preserved: {', '.join(found_diacritics)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Extraction failed: {e}\")\n",
    "\n",
    "print(\"\\n💡 Observations:\")\n",
    "print(\"   • UTF-8 encoding preserves all Croatian diacritics\")\n",
    "print(\"   • Metadata extraction provides useful document information\")\n",
    "print(\"   • Text structure (headers, lists) is preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = DocumentChunker(config)\n",
    "\n",
    "try:\n",
    "    chunks = chunker.chunk_document(\n",
    "        text=test_document,\n",
    "        source_file=\"zagreb_analysis.txt\",\n",
    "        strategy=\"sliding_window\"\n",
    "    )\n",
    "\n",
    "    print(f\"📊 Quality Analysis for {len(chunks)} chunks:\")\n",
    "    print()\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        content = chunk.content\n",
    "        metadata = chunk.metadata\n",
    "\n",
    "        # Analyze chunk quality\n",
    "        sentences = content.count('. ') + content.count('!') + content.count('?')\n",
    "        words = len(content.split())\n",
    "        has_header = any(line.isupper() or line.startswith('#') for line in content.split('\\n'))\n",
    "\n",
    "        print(f\"\udcc4 Chunk {i+1} (ID: {metadata['chunk_id'][:8]}...):\")\n",
    "        print(f\"   📏 Length: {len(content)} chars, {words} words, ~{sentences} sentences\")\n",
    "        print(f\"   \udd22 Index: {metadata['chunk_index']}/{metadata['total_chunks']}\")\n",
    "        print(f\"   🏷️  Language: {metadata.get('language', 'auto-detected')}\")\n",
    "        print(f\"   📄 Source: {metadata['source_file']}\")\n",
    "        if has_header:\n",
    "            print(f\"   📋 Contains headers/structure\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in chunking: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Croatian Text Cleaning Challenges\n",
    "\n",
    "### Why Clean Text?\n",
    "\n",
    "Raw extracted text often contains:\n",
    "- **Formatting artifacts**: Extra spaces, line breaks\n",
    "- **Non-printable characters**: Control characters, BOM\n",
    "- **Inconsistent whitespace**: Tabs, multiple spaces\n",
    "- **OCR errors**: From scanned documents\n",
    "- **Mixed languages**: Headers/footers in other languages\n",
    "\n",
    "### Croatian-Specific Cleaning:\n",
    "\n",
    "1. **Diacritic Normalization**: Ensure consistent diacritic representation\n",
    "2. **Case Handling**: Proper Croatian title case rules\n",
    "3. **Punctuation**: Handle Croatian-specific quotation marks\n",
    "4. **Number Formats**: Croatian uses comma for decimals (123,45)\n",
    "5. **Date Formats**: DD.MM.YYYY. format common in Croatia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test Croatian text cleaning capabilities\n",
    "print(\"🧹 CROATIAN TEXT CLEANING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create text cleaner with Croatian-specific configuration\n",
    "cleaning_config = TextCleaningConfig(\n",
    "    normalize_whitespace=True,\n",
    "    remove_extra_newlines=True,\n",
    "    normalize_diacritics=False,  # Keep Croatian diacritics!\n",
    "    lowercase=False,  # Preserve proper names\n",
    "    remove_punctuation=False,\n",
    "    min_word_length=2,\n",
    "    language='hr'  # Croatian-specific rules\n",
    ")\n",
    "\n",
    "cleaner = CroatianTextCleaner(cleaning_config)\n",
    "\n",
    "print(f\"⚙️ Text cleaner configured for Croatian:\")\n",
    "print(f\"   • Language: {cleaning_config.language}\")\n",
    "print(f\"   • Preserve diacritics: {not cleaning_config.normalize_diacritics}\")\n",
    "print(f\"   • Preserve case: {not cleaning_config.lowercase}\")\n",
    "print(f\"   • Min word length: {cleaning_config.min_word_length}\")\n",
    "\n",
    "# Test with messy Croatian text\n",
    "messy_text = \"\"\"\n",
    "\n",
    "   ZAGREB    -   GLAVNI GRAD\n",
    "\n",
    "\n",
    "Zagreb  je  glavni   grad  Republike   Hrvatske.\n",
    "    Nalazi    se   u   sjeverozapadnom    dijelu    zemlje.\n",
    "\n",
    "\n",
    "Stanovništvo:     ~800,000     stanovnika     (2021.)\n",
    "\n",
    "Površina:   641.4    km²\n",
    "\n",
    "\n",
    "VAŽNE   ČINJENICE:\n",
    "•    Osnovan    je    u    11.    stoljeću\n",
    "•  Glavni    grad   od    1991.  godine\n",
    "•    Sveučilište    osnovano    1669.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n📝 Original messy text:\")\n",
    "print(f\"   Length: {len(messy_text)} characters\")\n",
    "print(f\"   Lines: {messy_text.count(chr(10))} newlines\")\n",
    "print(f\"   Preview: {repr(messy_text[:100])}...\")\n",
    "\n",
    "# Clean the text\n",
    "try:\n",
    "    cleaned_result = cleaner.clean_text(messy_text)\n",
    "    cleaned_text = cleaned_result['text']\n",
    "\n",
    "    print(\"\\n✨ Cleaned text:\")\n",
    "    print(f\"   Length: {len(cleaned_text)} characters (reduced by {len(messy_text) - len(cleaned_text)})\")\n",
    "    print(f\"   Lines: {cleaned_text.count(chr(10))} newlines\")\n",
    "    print(f\"   Preview: {cleaned_text[:200]}...\")\n",
    "\n",
    "    # Show cleaning statistics\n",
    "    stats = cleaned_result['metadata']['cleaning_stats']\n",
    "    print(\"\\n📊 Cleaning Statistics:\")\n",
    "    for operation, count in stats.items():\n",
    "        if count > 0:\n",
    "            print(f\"   • {operation}: {count}\")\n",
    "\n",
    "    # Verify Croatian diacritics are preserved\n",
    "    croatian_chars = ['č', 'ć', 'š', 'ž', 'đ']\n",
    "    found_chars = [char for char in croatian_chars if char in cleaned_text]\n",
    "    if found_chars:\n",
    "        print(f\"\\n✓ Croatian diacritics preserved: {', '.join(found_chars)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Cleaning failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = \"paragraph\"\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = \"sentence\"\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = \"sliding_window\"  # Preserve structure\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = \"sentence\"\n",
    "        max_size = 200\n",
    "\n",
    "    chunker = DocumentChunker(language='hr')\n",
    "\n",
    "    try:\n",
    "        chunks = chunker.chunk_document(\n",
    "            text=content.strip(),\n",
    "            source_file=f\"{content_name.lower()}_sample.txt\",\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        print(f\"   📊 Strategy: {strategy}, Max size: {max_size}\")\n",
    "        print(f\"   📑 Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk.content[:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk.content)} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Chunking Strategies\n",
    "\n",
    "### Why Chunk Documents?\n",
    "\n",
    "Large documents must be split into smaller pieces because:\n",
    "- **Embedding limits**: Models have max token limits (512-1024 tokens)\n",
    "- **Search precision**: Smaller chunks = more focused results\n",
    "- **Context relevance**: Large chunks may contain irrelevant information\n",
    "- **Processing efficiency**: Smaller pieces are faster to process\n",
    "\n",
    "### Chunking Strategies:\n",
    "\n",
    "1. **Sentence-based**: Split on sentence boundaries (good for Croatian)\n",
    "2. **Fixed-size**: Split by character/token count\n",
    "3. **Paragraph-based**: Split on paragraph breaks\n",
    "4. **Semantic**: Split based on topic/meaning\n",
    "5. **Hybrid**: Combine multiple approaches\n",
    "\n",
    "### Croatian Considerations:\n",
    "\n",
    "- **Sentence detection**: Croatian punctuation patterns\n",
    "- **Long sentences**: Croatian can have very long complex sentences\n",
    "- **Paragraph structure**: Formal vs informal text differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        # Use chunk_document with source_file parameter\n",
    "        chunks = chunker.chunk_document(\n",
    "            content=content.strip(),\n",
    "            source_file=f\"sample_{content_name.lower()}.txt\",\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   📊 Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   📑 Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content_name, content in content_types.items():\n",
    "    print(f\"\udcd6 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        # Use chunk_document with source_file parameter\n",
    "        chunks = chunker.chunk_document(\n",
    "            content=content.strip(),\n",
    "            source_file=f\"sample_{content_name.lower()}.txt\",\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   \udcca Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   📑 Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunking with different types of Croatian content\n",
    "print(\"📚 CHUNKING DIFFERENT CONTENT TYPES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Different Croatian content types\n",
    "content_types = {\n",
    "    \"News Article\": \"\"\"\n",
    "ZAGREB, 15. srpnja 2023. - Hrvatski premijer najavio je nova ulaganja u infrastrukturu.\n",
    "\n",
    "Prema riječima premijera, projekti će uključivati modernizaciju cesta, željeznica i digitalne infrastrukture. \"Ovo su strateška ulaganja za budućnost Hrvatske\", rekao je premijer na tiskovnoj konferenciji.\n",
    "\n",
    "Ukupna vrijednost projekata procjenjuje se na 2,5 milijardi eura. Financiranje će biti osigurano iz EU fondova i državnog proračuna.\n",
    "\"\"\",\n",
    "\n",
    "    \"Academic Text\": \"\"\"\n",
    "Sintaksa hrvatskog jezika odlikuje se složenošću koja proizlazi iz bogate morfologije. Fleksijska priroda jezika omogućuje relativno slobodan red riječi, što se posebno očituje u poetskom diskursu.\n",
    "\n",
    "Slavenski jezici, uključujući hrvatski, karakterizira razvijen aspektualnost glagolski sistem. Perfektivnost i imperfektivnost glagola fundamentalne su kategorije koje utječu na temporalnu strukturu iskaza.\n",
    "\"\"\",\n",
    "\n",
    "    \"Recipe\": \"\"\"\n",
    "SARMA - tradicionalno zimsko jelo\n",
    "\n",
    "Potrebno:\n",
    "• 1 kg miješanog mesa\n",
    "• 1 glavica kiselog kupusa\n",
    "• 200g riže\n",
    "• 2 luka\n",
    "• Sol, papar, vegeta\n",
    "\n",
    "Priprema:\n",
    "1. Prokuhajte rižu na pola\n",
    "2. Pomiješajte meso s rižom i začinima\n",
    "3. Zamotajte u kupusove listove\n",
    "4. Kuhajte 2 sata na laganoj vatri\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for content_name, content in content_types.items():\n",
    "    print(f\"📖 {content_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Choose appropriate chunking strategy based on content type\n",
    "    if \"News\" in content_name:\n",
    "        strategy = ChunkingStrategy.PARAGRAPH\n",
    "        max_size = 200\n",
    "    elif \"Academic\" in content_name:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 150  # Shorter for complex sentences\n",
    "    elif \"Recipe\" in content_name:\n",
    "        strategy = ChunkingStrategy.HYBRID  # Preserve lists\n",
    "        max_size = 100\n",
    "    else:\n",
    "        strategy = ChunkingStrategy.SENTENCE\n",
    "        max_size = 200\n",
    "\n",
    "    config = ChunkingConfig(\n",
    "        strategy=strategy,\n",
    "        max_chunk_size=max_size,\n",
    "        overlap_size=20,\n",
    "        language='hr'\n",
    "    )\n",
    "\n",
    "    chunker = DocumentChunker(config)\n",
    "\n",
    "    try:\n",
    "        chunks = chunker.chunk_text(\n",
    "            text=content.strip(),\n",
    "            metadata={\"content_type\": content_name.lower(), \"language\": \"hr\"}\n",
    "        )\n",
    "\n",
    "        print(f\"   📊 Strategy: {strategy.value}, Max size: {max_size}\")\n",
    "        print(f\"   📑 Chunks created: {len(chunks)}\")\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            preview = chunk['content'][:60].replace('\\n', ' ')\n",
    "            print(f\"   {i+1}. {preview}... ({len(chunk['content'])} chars)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"🎯 Content-specific chunking improves RAG quality:\")\n",
    "print(\"   • News: Paragraph-based preserves story flow\")\n",
    "print(\"   • Academic: Sentence-based handles complexity\")\n",
    "print(\"   • Recipes: Hybrid preserves structured lists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Document Processing Pipeline\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "A complete document processing pipeline combines:\n",
    "1. **Document detection and loading**\n",
    "2. **Format-specific text extraction**\n",
    "3. **Croatian-aware text cleaning**\n",
    "4. **Intelligent document chunking**\n",
    "5. **Metadata preservation and enrichment**\n",
    "\n",
    "### Pipeline Benefits:\n",
    "- **Consistency**: Same processing for all documents\n",
    "- **Quality**: Each step improves text quality\n",
    "- **Scalability**: Can process hundreds of documents\n",
    "- **Traceability**: Track processing steps and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_complete(file_path, chunk_strategy=\"sentence\"):\n",
    "    \"\"\"Complete document processing pipeline.\"\"\"\n",
    "    print(f\"   \udcc4 Processing: {Path(file_path).name}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        print(\"   1️⃣ Text extraction...\")\n",
    "        extractor = DocumentExtractor()\n",
    "        raw_text, file_metadata = extractor.extract_text(file_path)\n",
    "        print(f\"      ✅ Extracted {len(raw_text)} characters\")\n",
    "\n",
    "        # Step 2: Clean text\n",
    "        print(\"   2️⃣ Text cleaning...\")\n",
    "        cleaner = MultilingualTextCleaner(language='hr')\n",
    "        cleaned_text, cleaning_stats = cleaner.clean_text(raw_text)\n",
    "        print(f\"      ✅ Cleaned: {cleaning_stats['characters_removed']} chars removed\")\n",
    "\n",
    "        # Step 3: Chunk text\n",
    "        print(\"   3️⃣ Text chunking...\")\n",
    "\n",
    "        # Map strategy names to valid options\n",
    "        strategy_mapping = {\n",
    "            \"sentence\": \"sentence\",\n",
    "            \"paragraph\": \"paragraph\",\n",
    "            \"sliding_window\": \"sliding_window\"\n",
    "        }\n",
    "\n",
    "        strategy = strategy_mapping.get(chunk_strategy, \"sentence\")\n",
    "        chunker = DocumentChunker(language='hr')\n",
    "\n",
    "        chunks = chunker.chunk_document(\n",
    "            text=cleaned_text,\n",
    "            source_file=str(file_path),\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        print(f\"      ✅ Created {len(chunks)} chunks\")\n",
    "\n",
    "        # Step 4: Quality check\n",
    "        print(\"   4️⃣ Quality check...\")\n",
    "        chunk_sizes = [len(chunk.content) for chunk in chunks]\n",
    "        avg_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0\n",
    "\n",
    "        # Check for quality issues\n",
    "        quality_issues = []\n",
    "        if avg_size < 50:\n",
    "            quality_issues.append(\"chunks too small\")\n",
    "        if max(chunk_sizes) if chunk_sizes else 0 > 500:\n",
    "            quality_issues.append(\"some chunks too large\")\n",
    "        if not any('č' in chunk.content or 'ć' in chunk.content or\n",
    "                  'š' in chunk.content or 'ž' in chunk.content or\n",
    "                  'đ' in chunk.content for chunk in chunks):\n",
    "            if any(c in raw_text for c in 'čćšžđ'):\n",
    "                quality_issues.append(\"Croatian diacritics lost\")\n",
    "\n",
    "        if quality_issues:\n",
    "            print(f\"      ⚠️ Quality issues: {', '.join(quality_issues)}\")\n",
    "        else:\n",
    "            print(f\"      ✅ Quality check passed\")\n",
    "\n",
    "        print(f\"      📊 Avg chunk size: {avg_size:.1f} chars\")\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ Pipeline failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Multilingual Document Examples\n",
    "\n",
    "### 🌍 Real-World Document Processing Scenarios\n",
    "\n",
    "Here we demonstrate processing diverse multilingual documents that reflect actual use cases in Croatian business and government environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced multilingual document examples for processing\n",
    "multilingual_documents = {\n",
    "    \"🏛️ Croatian Legal Document\": {\n",
    "        \"content\": \"\"\"ODLUKA VLADE REPUBLIKE HRVATSKE\n",
    "O IZMJENI ZAKONA O RADU\n",
    "\n",
    "Klasa: 022-03/25-01/89\n",
    "Urbroj: 50301-01-25-4\n",
    "Zagreb, 15. lipnja 2025.\n",
    "\n",
    "Na temelju članka 89. Ustava Republike Hrvatske, Vlada Republike Hrvatske na sjednici održanoj 15. lipnja 2025. godine donosi\n",
    "\n",
    "ODLUKU\n",
    "O IZMJENI ZAKONA O RADU\n",
    "\n",
    "Članak 1.\n",
    "U Zakonu o radu (»Narodne novine«, broj 93/14, 127/17, 98/19 i 151/22) mijenja se članak 142. koji glasi:\n",
    "\n",
    "»Minimalna mjesečna plaća za puno radno vrijeme iznosi 721,79 EUR.\n",
    "Minimalna mjesečna plaća uvjetovana je indeksom potrošačkih cijena.\n",
    "Vlada Republike Hrvatske donosi uredbu o visini minimalne plaće.«\n",
    "\n",
    "Članak 2.\n",
    "Ova Odluka stupa na snagu 1. srpnja 2025. godine.\n",
    "\n",
    "Predsjednik Vlade Republike Hrvatske\n",
    "Andrej Plenković\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"legal\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"formal_language\", \"legal_terminology\", \"official_formatting\", \"currency_amounts\", \"dates\"]\n",
    "    },\n",
    "\n",
    "    \"💼 Croatian Business Report\": {\n",
    "        \"content\": \"\"\"IZVJEŠĆE O POSLOVANJU ZA Q3 2025\n",
    "TECH SOLUTIONS d.o.o.\n",
    "\n",
    "SAŽETAK FINANCIJSKIH REZULTATA\n",
    "\n",
    "Ukupni prihodi: 2.450.000,00 EUR (+15,3% u odnosu na Q3 2024)\n",
    "Neto dobit: 367.500,00 EUR (+22,7% YoY)\n",
    "EBITDA: 490.000,00 EUR (20,0% marža)\n",
    "\n",
    "KLJUČNI INDIKATORI PERFORMANSI (KPI)\n",
    "• Broj zaposlenih: 47 (+6 nova zaposlenja)\n",
    "• Customer Satisfaction Score: 4.7/5.0\n",
    "• Net Promoter Score (NPS): +65\n",
    "• Retention rate: 94,2%\n",
    "\n",
    "PROJEKTI I INOVACIJE\n",
    "Tijekom Q3 implementirali smo RAG (Retrieval-Augmented Generation) sustav za automatizaciju customer support-a. Sustav koristi BGE-M3 embeddings i Ollama lokalnu infrastrukturu.\n",
    "\n",
    "Rezultati:\n",
    "- 40% smanjenje response time-a\n",
    "- 15% povećanje customer satisfaction\n",
    "- Ušteda od 25.000 EUR mjesečno na operativnim troškovima\n",
    "\n",
    "OUTLOOK ZA Q4 2025\n",
    "Očekujemo daljnji rast prihoda od 12-18% te lansiranje novog AI-powered produkta.\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"business\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"features\": [\"mixed_terminology\", \"financial_data\", \"percentages\", \"technical_terms\", \"english_acronyms\"]\n",
    "    },\n",
    "\n",
    "    \"🔬 English Technical Documentation\": {\n",
    "        \"content\": \"\"\"RAG SYSTEM ARCHITECTURE SPECIFICATION\n",
    "Version 2.1.3 | Build Date: September 5, 2025\n",
    "\n",
    "OVERVIEW\n",
    "This document outlines the technical architecture for a multilingual Retrieval-Augmented Generation (RAG) system supporting Croatian and English languages.\n",
    "\n",
    "SYSTEM COMPONENTS\n",
    "\n",
    "1. DOCUMENT PROCESSING PIPELINE\n",
    "   - Input formats: PDF, DOCX, TXT\n",
    "   - Encoding detection: UTF-8, Windows-1250, ISO-8859-2\n",
    "   - Language detection: Pattern-based Croatian/English classification\n",
    "   - Text extraction: pypdf, python-docx libraries\n",
    "   - Cleaning: Unicode normalization, whitespace handling\n",
    "\n",
    "2. CHUNKING STRATEGY\n",
    "   - Sentence-based: 100-400 characters with 20-50 overlap\n",
    "   - Paragraph-based: Semantic boundary preservation\n",
    "   - Hybrid: Content-aware splitting for complex documents\n",
    "   - Language-aware: Croatian morphology considerations\n",
    "\n",
    "3. VECTOR DATABASE\n",
    "   - Embeddings: BGE-M3 (1024 dimensions)\n",
    "   - Storage: ChromaDB with persistence\n",
    "   - Indexing: HNSW algorithm for similarity search\n",
    "   - Metadata: Language tags, source references, timestamps\n",
    "\n",
    "4. RETRIEVAL SYSTEM\n",
    "   - Dense retrieval: Semantic similarity via embeddings\n",
    "   - Sparse retrieval: BM25 with multilingual tokenization\n",
    "   - Hybrid fusion: Weighted combination (0.7 dense + 0.3 sparse)\n",
    "   - Reranking: Cross-encoder for result refinement\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "- Query latency: <200ms (p95)\n",
    "- Retrieval accuracy: >85% (HR), >92% (EN)\n",
    "- Storage efficiency: 4.2MB per 1000 documents\n",
    "- Throughput: 15 queries/second sustained\"\"\",\n",
    "        \"language\": \"en\",\n",
    "        \"domain\": \"technical\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"technical_specifications\", \"performance_metrics\", \"system_architecture\", \"acronyms\", \"version_numbers\"]\n",
    "    },\n",
    "\n",
    "    \"🌍 Mixed Language Business Communication\": {\n",
    "        \"content\": \"\"\"MEETINGR ZAPISNIK / MEETING MINUTES\n",
    "Tech Solutions d.o.o. - Q3 Strategy Review\n",
    "Datum/Date: 15. rujan 2025. / September 15, 2025\n",
    "\n",
    "SUDIONICI/ATTENDEES:\n",
    "- Marko Horvat (CEO)\n",
    "- Sarah Johnson (CTO)\n",
    "- Ana Novak (Head of Operations)\n",
    "- James Wilson (Product Manager)\n",
    "\n",
    "AGENDA ITEMS:\n",
    "\n",
    "1. Q3 PERFORMANCE REVIEW\n",
    "Marko: \"Rezultati za Q3 su odlični - 15.3% rast u odnosu na prošlu godinu.\"\n",
    "Sarah: \"The RAG implementation exceeded our expectations. Response time improved by 40%.\"\n",
    "\n",
    "2. TECHNICAL ROADMAP\n",
    "Sarah: \"We need to expand our LLM capabilities. Predlažem da implementiramo nova multilingual models.\"\n",
    "James: \"I agree. Our customer base is 60% Croatian, 35% English, 5% other languages.\"\n",
    "\n",
    "3. RESOURCE ALLOCATION\n",
    "Ana: \"Za Q4 trebamo zaposliti još 3 developera i 2 data scientista.\"\n",
    "Sarah: \"Budget approval needed for additional GPU infrastructure - approximately €45,000.\"\n",
    "\n",
    "4. NEXT STEPS\n",
    "- Implement BGE-M3 embeddings for better Croatian support\n",
    "- Hire additional technical staff (deadline: 30. listopad 2025.)\n",
    "- Present findings to board (meeting scheduled November 12th, 2025)\n",
    "\n",
    "CLOSING REMARKS:\n",
    "Marko: \"Excellent progress team. Let's maintain this momentum kroz Q4.\"\n",
    "\n",
    "SLJEDEĆI SASTANAK/NEXT MEETING: 30. rujan 2025. / September 30, 2025\"\"\",\n",
    "        \"language\": \"mixed\",\n",
    "        \"domain\": \"business_communication\",\n",
    "        \"complexity\": \"medium\",\n",
    "        \"features\": [\"code_switching\", \"bilingual_formatting\", \"meeting_structure\", \"mixed_dates\", \"currency_amounts\"]\n",
    "    },\n",
    "\n",
    "    \"📊 Croatian Government Statistics\": {\n",
    "        \"content\": \"\"\"DRŽAVNI ZAVOD ZA STATISTIKU REPUBLIKE HRVATSKE\n",
    "STATISTIČKO IZVJEŠĆE BR. 1.2.3/2025\n",
    "\n",
    "DEMOGRAFSKI TRENDOVI U HRVATSKOJ - RUJAN 2025\n",
    "\n",
    "STANOVNIŠTVO\n",
    "Ukupno stanovništvo: 3.871.833 (-0,8% u odnosu na 2024.)\n",
    "Prirodni prirast: -1,2 ‰\n",
    "Migracijski saldo: +0,4 ‰\n",
    "\n",
    "REGIONALNA DISTRIBUCIJA\n",
    "• Zagreb (grad): 769.944 stanovnika (+0,2%)\n",
    "• Split-Dalmatinska županija: 454.798 (-1,1%)\n",
    "• Primorsko-goranska županija: 296.195 (-0,9%)\n",
    "• Osječko-baranjska županija: 305.032 (-1,4%)\n",
    "\n",
    "EKONOMSKI INDIKATORI\n",
    "Prosječna neto plaća: 1.235,67 EUR (+7,2% YoY)\n",
    "Stopa nezaposlenosti: 6,1% (-0,8 postotnih bodova)\n",
    "Inflacija (mjesečno): 2,4%\n",
    "BDP per capita: 16.247 EUR\n",
    "\n",
    "OBRAZOVANJE\n",
    "Stopa pismenosti: 99,2%\n",
    "Visokoškolska naobrazba: 28,7% (+1,2 p.p.)\n",
    "STEM diplomanti: 23,4% ukupnih diplomanata\n",
    "\n",
    "NAPOMENE METODOLOGIJE\n",
    "Podaci se temelje na registru stanovništva i anketi radne snage.\n",
    "Confidence interval: 95%\n",
    "Margin of error: ±0,3%\n",
    "\n",
    "Objavljeno: 5. rujan 2025.\n",
    "Sljedeće izvješće: 5. prosinac 2025.\"\"\",\n",
    "        \"language\": \"hr\",\n",
    "        \"domain\": \"statistics\",\n",
    "        \"complexity\": \"high\",\n",
    "        \"features\": [\"statistical_data\", \"percentages\", \"regional_data\", \"economic_indicators\", \"methodological_notes\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🌍 Advanced Multilingual Document Examples Prepared:\")\n",
    "print(\"=\" * 65)\n",
    "for doc_name, doc_data in multilingual_documents.items():\n",
    "    print(f\"{doc_name}\")\n",
    "    print(f\"   Language: {doc_data['language']} | Domain: {doc_data['domain']}\")\n",
    "    print(f\"   Complexity: {doc_data['complexity']} | Length: {len(doc_data['content'])} chars\")\n",
    "    print(f\"   Features: {', '.join(doc_data['features'])}\")\n",
    "    print(f\"   Preview: {doc_data['content'][:80].replace(chr(10), ' ')}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive multilingual document processing demonstration\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def detect_document_language(text):\n",
    "    \"\"\"Simple language detection for demonstration.\"\"\"\n",
    "    croatian_indicators = len(re.findall(r'[čćžšđ]', text.lower()))\n",
    "    croatian_words = len(re.findall(r'\\b(je|su|za|na|u|od|do|se|i|a|koji|koja|koje|gdje|kada|kako|što)\\b', text.lower()))\n",
    "    english_indicators = len(re.findall(r'\\b(the|and|or|of|in|to|for|with|on|at|by|from)\\b', text.lower()))\n",
    "\n",
    "    total_words = len(text.split())\n",
    "    if total_words == 0:\n",
    "        return \"unknown\"\n",
    "\n",
    "    croatian_score = (croatian_indicators * 3 + croatian_words) / total_words\n",
    "    english_score = english_indicators / total_words\n",
    "\n",
    "    if croatian_score > 0.1 and english_score > 0.05:\n",
    "        return \"mixed\"\n",
    "    elif croatian_score > english_score:\n",
    "        return \"hr\"\n",
    "    elif english_score > 0.05:\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def extract_document_features(text, expected_features):\n",
    "    \"\"\"Extract and validate document features.\"\"\"\n",
    "    found_features = []\n",
    "\n",
    "    # Check for each expected feature\n",
    "    if \"formal_language\" in expected_features:\n",
    "        formal_patterns = r'\\b(Vlada|Republika|Odluka|članak|temelju|Ustava)\\b'\n",
    "        if re.search(formal_patterns, text, re.IGNORECASE):\n",
    "            found_features.append(\"formal_language\")\n",
    "\n",
    "    if \"financial_data\" in expected_features:\n",
    "        financial_patterns = r'\\d+[.,]\\d+\\s*(EUR|€|\\%)'\n",
    "        if re.search(financial_patterns, text):\n",
    "            found_features.append(\"financial_data\")\n",
    "\n",
    "    if \"technical_terms\" in expected_features:\n",
    "        tech_patterns = r'\\b(RAG|API|BGE-M3|embeddings|vector|database|algorithm|pipeline)\\b'\n",
    "        if re.search(tech_patterns, text, re.IGNORECASE):\n",
    "            found_features.append(\"technical_terms\")\n",
    "\n",
    "    if \"dates\" in expected_features:\n",
    "        date_patterns = r'\\d{1,2}\\.\\s*(siječnja|veljače|ožujka|travnja|svibnja|lipnja|srpnja|kolovoza|rujna|listopada|studenoga|prosinca|\\d+)\\s*\\d{4}|\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s*\\d{4}'\n",
    "        if re.search(date_patterns, text):\n",
    "            found_features.append(\"dates\")\n",
    "\n",
    "    if \"mixed_terminology\" in expected_features:\n",
    "        mixed_patterns = r'\\b(customer|support|response|time|satisfaction|score)\\b.*\\b(sustav|implementacija|rezultati|troškovi)\\b|\\b(sustav|implementacija|rezultati|troškovi)\\b.*\\b(customer|support|response|time|satisfaction|score)\\b'\n",
    "        if re.search(mixed_patterns, text, re.IGNORECASE | re.DOTALL):\n",
    "            found_features.append(\"mixed_terminology\")\n",
    "\n",
    "    if \"code_switching\" in expected_features:\n",
    "        # Look for language switches within sentences\n",
    "        sentences = text.split('.')\n",
    "        for sentence in sentences:\n",
    "            croatian_words = len(re.findall(r'\\b(je|su|za|na|u|od|trebamo|možemo|godina)\\b', sentence.lower()))\n",
    "            english_words = len(re.findall(r'\\b(the|and|we|need|can|year|meeting|next)\\b', sentence.lower()))\n",
    "            if croatian_words > 0 and english_words > 0:\n",
    "                found_features.append(\"code_switching\")\n",
    "                break\n",
    "\n",
    "    return found_features\n",
    "\n",
    "# Process all multilingual documents\n",
    "print(\"📊 Comprehensive Multilingual Document Processing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "processing_results = []\n",
    "\n",
    "for doc_name, doc_data in multilingual_documents.items():\n",
    "    print(f\"\\n🔍 Processing: {doc_name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    content = doc_data['content']\n",
    "    expected_lang = doc_data['language']\n",
    "    expected_features = doc_data['features']\n",
    "\n",
    "    # Language detection\n",
    "    detected_lang = detect_document_language(content)\n",
    "    lang_match = detected_lang == expected_lang\n",
    "\n",
    "    # Feature extraction\n",
    "    found_features = extract_document_features(content, expected_features)\n",
    "    feature_accuracy = len(found_features) / len(expected_features) if expected_features else 0\n",
    "\n",
    "    # Document statistics\n",
    "    word_count = len(content.split())\n",
    "    char_count = len(content)\n",
    "    sentence_count = len([s for s in content.split('.') if s.strip()])\n",
    "\n",
    "    # Chunking simulation (sentence-based)\n",
    "    sentences = [s.strip() + '.' for s in content.split('.') if s.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk + sentence) <= 300:  # Target chunk size\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'name': doc_name,\n",
    "        'expected_lang': expected_lang,\n",
    "        'detected_lang': detected_lang,\n",
    "        'lang_match': lang_match,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'chunk_count': len(chunks),\n",
    "        'expected_features': expected_features,\n",
    "        'found_features': found_features,\n",
    "        'feature_accuracy': feature_accuracy,\n",
    "        'domain': doc_data['domain'],\n",
    "        'complexity': doc_data['complexity']\n",
    "    }\n",
    "    processing_results.append(result)\n",
    "\n",
    "    # Display results\n",
    "    status = \"✅\" if lang_match else \"❌\"\n",
    "    print(f\"{status} Language Detection: {detected_lang} (expected: {expected_lang})\")\n",
    "    print(f\"📏 Document Stats: {word_count} words, {sentence_count} sentences, {char_count} chars\")\n",
    "    print(f\"🔪 Chunking Result: {len(chunks)} chunks created\")\n",
    "    print(f\"🏷️  Feature Accuracy: {feature_accuracy:.1%} ({len(found_features)}/{len(expected_features)})\")\n",
    "    print(f\"   Expected: {', '.join(expected_features[:3])}{'...' if len(expected_features) > 3 else ''}\")\n",
    "    print(f\"   Found: {', '.join(found_features[:3])}{'...' if len(found_features) > 3 else ''}\")\n",
    "\n",
    "    # Show sample chunk\n",
    "    if chunks:\n",
    "        sample_chunk = chunks[0][:100] + \"...\" if len(chunks[0]) > 100 else chunks[0]\n",
    "        print(f\"📄 Sample Chunk: {sample_chunk}\")\n",
    "\n",
    "# Overall processing statistics\n",
    "print(f\"\\n🎯 Overall Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_docs = len(processing_results)\n",
    "correct_lang_detection = sum(1 for r in processing_results if r['lang_match'])\n",
    "avg_feature_accuracy = sum(r['feature_accuracy'] for r in processing_results) / total_docs\n",
    "total_chunks = sum(r['chunk_count'] for r in processing_results)\n",
    "avg_chunk_per_doc = total_chunks / total_docs\n",
    "\n",
    "print(f\"📊 Language Detection Accuracy: {correct_lang_detection}/{total_docs} ({correct_lang_detection/total_docs:.1%})\")\n",
    "print(f\"🎯 Average Feature Detection: {avg_feature_accuracy:.1%}\")\n",
    "print(f\"🔪 Total Chunks Created: {total_chunks}\")\n",
    "print(f\"📈 Average Chunks per Document: {avg_chunk_per_doc:.1f}\")\n",
    "\n",
    "# Domain and complexity analysis\n",
    "domain_stats = Counter(r['domain'] for r in processing_results)\n",
    "complexity_stats = Counter(r['complexity'] for r in processing_results)\n",
    "\n",
    "print(f\"\\n📂 Domain Distribution: {dict(domain_stats)}\")\n",
    "print(f\"⚖️  Complexity Distribution: {dict(complexity_stats)}\")\n",
    "\n",
    "# Language-specific performance\n",
    "lang_performance = {}\n",
    "for result in processing_results:\n",
    "    lang = result['expected_lang']\n",
    "    if lang not in lang_performance:\n",
    "        lang_performance[lang] = {'correct': 0, 'total': 0, 'chunks': 0}\n",
    "    lang_performance[lang]['total'] += 1\n",
    "    lang_performance[lang]['chunks'] += result['chunk_count']\n",
    "    if result['lang_match']:\n",
    "        lang_performance[lang]['correct'] += 1\n",
    "\n",
    "print(f\"\\n🌍 Language-Specific Performance:\")\n",
    "for lang, stats in lang_performance.items():\n",
    "    accuracy = stats['correct'] / stats['total']\n",
    "    avg_chunks = stats['chunks'] / stats['total']\n",
    "    print(f\"   {lang}: {accuracy:.1%} accuracy, {avg_chunks:.1f} avg chunks/doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "print(\"\ude80 Testing complete pipeline on our sample documents:\")\n",
    "print()\n",
    "\n",
    "all_processed_chunks = []\n",
    "\n",
    "for filename, file_path in list(sample_files.items())[:2]:  # Process first 2 files\n",
    "    chunks = process_document_complete(file_path, chunk_strategy=\"sentence\")\n",
    "    all_processed_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\n\udcca Pipeline Summary:\")\n",
    "print(f\"   📄 Documents processed: 2\")\n",
    "print(f\"   📑 Total chunks created: {len(all_processed_chunks)}\")\n",
    "print(f\"   📏 Average chunk size: {sum(len(chunk.content) for chunk in all_processed_chunks) / len(all_processed_chunks):.1f} chars\")\n",
    "\n",
    "# Language detection check\n",
    "croatian_chunks = [chunk for chunk in all_processed_chunks\n",
    "                  if any(c in chunk.content for c in 'čćšžđ')]\n",
    "print(f\"   🇭🇷 Chunks with Croatian diacritics: {len(croatian_chunks)}\")\n",
    "\n",
    "print(\"\\n✅ Complete multilingual pipeline working perfectly!\")\n",
    "print(\"🌍 Ready for production Croatian document processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "🎯 **Document Processing is Critical**:\n",
    "- Quality processing = better RAG results\n",
    "- Each step (extract→clean→chunk) adds value\n",
    "- Bad preprocessing ruins the entire pipeline\n",
    "\n",
    "🇭🇷 **Croatian Language Considerations**:\n",
    "- Always preserve diacritics (č, ć, š, ž, đ)\n",
    "- Handle various encodings (UTF-8, Windows-1250)\n",
    "- Use Croatian-aware sentence splitting\n",
    "- Consider regional variations and dialects\n",
    "\n",
    "📝 **Chunking Strategy Matters**:\n",
    "- Sentence-based: Best for most Croatian content\n",
    "- Paragraph-based: Good for structured documents\n",
    "- Hybrid: Best for complex formats\n",
    "- Always include overlap for context preservation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "✅ **Text Extraction**:\n",
    "- Auto-detect encoding for legacy documents\n",
    "- Preserve file metadata for traceability\n",
    "- Handle extraction errors gracefully\n",
    "\n",
    "✅ **Text Cleaning**:\n",
    "- Use conservative cleaning for Croatian\n",
    "- Never normalize Croatian diacritics\n",
    "- Preserve proper names and technical terms\n",
    "- Normalize whitespace and quotes\n",
    "\n",
    "✅ **Document Chunking**:\n",
    "- Aim for 100-400 characters per chunk\n",
    "- Use 20-50 character overlap\n",
    "- Respect sentence boundaries\n",
    "- Include chunk metadata for debugging\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. ✅ **Document Processing** - Just completed!\n",
    "2. ✅ **Vector Database** - Already done\n",
    "3. ⏳ **Retrieval System** - Next step\n",
    "4. ⏳ **Generation** - Local LLM integration\n",
    "5. ⏳ **Complete Pipeline** - End-to-end integration\n",
    "\n",
    "The document processing pipeline creates clean, well-structured chunks that are ready for embedding and storage in our vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up our temporary files\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    if temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"🧹 Cleaned up temporary directory: {temp_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Warning: Could not clean up temp directory: {e}\")\n",
    "\n",
    "print(\"\\n🎉 Document Processing Learning Complete!\")\n",
    "print(\"\\n📚 What we learned:\")\n",
    "print(\"   • Text extraction from multiple formats\")\n",
    "print(\"   • Croatian-specific cleaning challenges\")\n",
    "print(\"   • Chunking strategies and their impact\")\n",
    "print(\"   • Complete processing pipeline design\")\n",
    "print(\"\\n➡️  Ready for the next step: Vector Database integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
