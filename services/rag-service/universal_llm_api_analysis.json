{
  "analysis_overview": {
    "objective": "Create a universal LLM provider interface that can work with OpenAI GPT, Google Gemini, and Anthropic Claude APIs",
    "goal": "Maximum flexibility and provider independence without vendor lock-in",
    "approach": "Abstract common patterns while handling provider-specific differences"
  },

  "api_comparison": {
    "openai_gpt": {
      "endpoint": "/v1/chat/completions",
      "method": "POST",
      "authentication": {
        "type": "Bearer token",
        "header": "Authorization: Bearer $OPENAI_API_KEY"
      },
      "request_format": {
        "required_fields": {
          "model": "string (e.g., 'gpt-4o-2024-08-06')",
          "messages": "array of message objects"
        },
        "optional_fields": {
          "system": "string (via messages array with role='system')",
          "max_tokens": "integer",
          "temperature": "float (0.0-2.0)",
          "top_p": "float (0.0-1.0)",
          "stream": "boolean",
          "stop": "string or array of strings",
          "presence_penalty": "float (-2.0-2.0)",
          "frequency_penalty": "float (-2.0-2.0)"
        },
        "message_structure": {
          "role": "enum: 'system', 'user', 'assistant'",
          "content": "string or array of content objects"
        }
      },
      "response_format": {
        "non_streaming": {
          "id": "string",
          "object": "chat.completion",
          "created": "integer (timestamp)",
          "model": "string",
          "choices": [
            {
              "index": "integer",
              "message": {
                "role": "assistant",
                "content": "string"
              },
              "finish_reason": "enum: stop, length, content_filter"
            }
          ],
          "usage": {
            "prompt_tokens": "integer",
            "completion_tokens": "integer",
            "total_tokens": "integer"
          }
        },
        "streaming": {
          "format": "Server-Sent Events",
          "event_structure": "data: {\"choices\": [{\"delta\": {\"content\": \"text\"}}]}"
        }
      }
    },

    "google_gemini": {
      "endpoint": "/v1beta/models/{model}:generateContent",
      "method": "POST",
      "authentication": {
        "type": "API key",
        "header": "x-goog-api-key: $GEMINI_API_KEY"
      },
      "request_format": {
        "required_fields": {
          "contents": "array of content objects"
        },
        "optional_fields": {
          "systemInstruction": "object with parts array",
          "generationConfig": {
            "temperature": "float",
            "topP": "float",
            "topK": "integer",
            "maxOutputTokens": "integer",
            "responseMimeType": "string"
          },
          "safetySettings": "array of safety configurations"
        },
        "content_structure": {
          "role": "enum: 'user', 'model'",
          "parts": [
            {
              "text": "string"
            }
          ]
        }
      },
      "response_format": {
        "non_streaming": {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "string"
                  }
                ],
                "role": "model"
              },
              "finishReason": "enum: STOP, MAX_TOKENS, SAFETY, RECITATION",
              "safetyRatings": "array"
            }
          ],
          "usageMetadata": {
            "promptTokenCount": "integer",
            "candidatesTokenCount": "integer",
            "totalTokenCount": "integer"
          }
        },
        "streaming": {
          "format": "Server-Sent Events",
          "event_structure": "data: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"chunk\"}]}}]}"
        }
      }
    },

    "anthropic_claude": {
      "endpoint": "/v1/messages",
      "method": "POST",
      "authentication": {
        "type": "API key",
        "header": "x-api-key: $ANTHROPIC_API_KEY"
      },
      "request_format": {
        "required_fields": {
          "model": "string (e.g., 'claude-3-5-sonnet-20241022')",
          "max_tokens": "integer",
          "messages": "array of message objects"
        },
        "optional_fields": {
          "system": "string or array of content objects",
          "temperature": "float (0.0-1.0)",
          "top_p": "float (0.0-1.0)",
          "stream": "boolean",
          "stop_sequences": "array of strings"
        },
        "message_structure": {
          "role": "enum: 'user', 'assistant'",
          "content": "string or array of content objects"
        }
      },
      "response_format": {
        "non_streaming": {
          "id": "string",
          "type": "message",
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "string"
            }
          ],
          "model": "string",
          "stop_reason": "enum: end_turn, max_tokens, stop_sequence",
          "stop_sequence": "string or null",
          "usage": {
            "input_tokens": "integer",
            "output_tokens": "integer"
          }
        },
        "streaming": {
          "format": "Server-Sent Events",
          "event_structure": "data: {\"type\": \"content_block_delta\", \"delta\": {\"text\": \"chunk\"}}"
        }
      }
    }
  },

  "key_differences": {
    "authentication": {
      "openai": "Bearer token in Authorization header",
      "gemini": "API key in x-goog-api-key header",
      "claude": "API key in x-api-key header"
    },
    "system_prompts": {
      "openai": "Via messages array with role='system'",
      "gemini": "Via systemInstruction object",
      "claude": "Via dedicated system field"
    },
    "message_roles": {
      "openai": "system, user, assistant",
      "gemini": "user, model (no system role in messages)",
      "claude": "user, assistant (system separate)"
    },
    "response_structure": {
      "openai": "choices[0].message.content",
      "gemini": "candidates[0].content.parts[0].text",
      "claude": "content[0].text"
    },
    "streaming_format": {
      "openai": "choices[0].delta.content",
      "gemini": "candidates[0].content.parts[0].text",
      "claude": "delta.text (in content_block_delta events)"
    },
    "token_limits": {
      "openai": "max_tokens (optional)",
      "gemini": "generationConfig.maxOutputTokens (optional)",
      "claude": "max_tokens (required)"
    }
  },

  "universal_interface_design": {
    "core_abstraction": {
      "name": "UniversalLLMProvider",
      "purpose": "Provider-agnostic interface for LLM interactions",
      "config_driven": "All provider differences handled through configuration"
    },
    "standardized_request_format": {
      "model": "string - provider-specific model name",
      "messages": [
        {
          "role": "enum: system, user, assistant",
          "content": "string - message text"
        }
      ],
      "system_prompt": "string - optional system instruction",
      "max_tokens": "integer - optional response limit",
      "temperature": "float - optional creativity control (0.0-1.0)",
      "stream": "boolean - optional streaming enable",
      "stop_sequences": "array of strings - optional stop conditions"
    },
    "standardized_response_format": {
      "id": "string - response identifier",
      "content": "string - generated text",
      "finish_reason": "enum: completed, max_tokens, stopped",
      "usage": {
        "input_tokens": "integer",
        "output_tokens": "integer",
        "total_tokens": "integer"
      },
      "provider": "string - which LLM provider was used",
      "model": "string - specific model used"
    }
  },

  "implementation_strategy": {
    "adapter_pattern": {
      "base_interface": "LLMProvider protocol/interface",
      "concrete_adapters": [
        "OpenAIAdapter",
        "GeminiAdapter",
        "ClaudeAdapter"
      ],
      "responsibility": "Each adapter translates universal format to provider-specific format"
    },
    "configuration_approach": {
      "provider_configs": "Separate config sections per provider",
      "dynamic_selection": "Runtime provider selection based on config/preferences",
      "fallback_chain": "Primary -> Secondary -> Tertiary provider support"
    },
    "error_handling": {
      "provider_failures": "Automatic fallback to next configured provider",
      "rate_limiting": "Exponential backoff with provider switching",
      "authentication_errors": "Clear error messages with provider identification"
    }
  },

  "config_structure": {
    "providers": {
      "primary": "string - preferred provider name",
      "fallback_order": ["secondary", "tertiary"],
      "openai": {
        "api_key": "${OPENAI_API_KEY}",
        "base_url": "https://api.openai.com",
        "model": "gpt-4o-2024-08-06",
        "endpoint": "/v1/chat/completions",
        "auth_header": "Authorization",
        "auth_prefix": "Bearer ",
        "response_format": "openai"
      },
      "gemini": {
        "api_key": "${GEMINI_API_KEY}",
        "base_url": "https://generativelanguage.googleapis.com",
        "model": "gemini-2.5-flash",
        "endpoint": "/v1beta/models/{model}:generateContent",
        "auth_header": "x-goog-api-key",
        "auth_prefix": "",
        "response_format": "gemini"
      },
      "claude": {
        "api_key": "${ANTHROPIC_API_KEY}",
        "base_url": "https://api.anthropic.com",
        "model": "claude-3-5-sonnet-20241022",
        "endpoint": "/v1/messages",
        "auth_header": "x-api-key",
        "auth_prefix": "",
        "response_format": "anthropic"
      }
    }
  },

  "language_control_implementation": {
    "system_prompt_templates": {
      "croatian": "Ti si pomoćni asistent koji odgovara ISKLJUČIVO na hrvatskom jeziku. Bez obzira na kontekst, uvijek odgovori na hrvatskom. Koristi dane informacije da daš precizan odgovor.",
      "english": "You are a helpful assistant who responds EXCLUSIVELY in English. Regardless of context, always respond in English. Use the given information to provide a precise answer."
    },
    "provider_mapping": {
      "openai": "Add system message to messages array",
      "gemini": "Use systemInstruction field",
      "claude": "Use system field"
    }
  },

  "migration_benefits": {
    "flexibility": "Easy switching between providers without code changes",
    "reliability": "Automatic failover if primary provider is down",
    "cost_optimization": "Use different providers for different use cases",
    "feature_access": "Leverage unique capabilities of different providers",
    "vendor_independence": "No lock-in to any single provider",
    "language_control": "Clean language handling without hard-coded patterns"
  },

  "implementation_files": {
    "core_interface": "src/llm/universal_provider.py",
    "adapters": [
      "src/llm/adapters/openai_adapter.py",
      "src/llm/adapters/gemini_adapter.py",
      "src/llm/adapters/claude_adapter.py"
    ],
    "config_models": "src/llm/provider_models.py",
    "factory": "src/llm/provider_factory.py",
    "config_updates": [
      "config/providers.toml",
      "src/utils/config_validator.py"
    ]
  }
}